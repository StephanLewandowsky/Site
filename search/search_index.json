{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Home Paul Garrett is a PhD candidate at the University of Newcastle, Australia and a visiting academic at the University of Melbourne. Paul studies cognitive psychology and human decsion making from the perspectives of psychophysics, computational modelling and systems analysis. During his PhD, Paul used R, Python and Matlab to develop mathematical tools and models to diagnose changes in the processing structure and efficiency of cognitive systems (e.g., the visual system). Paul has created educational materials for undergraduate and graduate students, teaching on topics including scientific writing, statistics, coding, professional development, and cognitive psychology. Paul is passionate about scientific communication, education and mentorship. He is a strong supporter of open science (data, code and publications), and aims to make his work and resources readily available to other scientists and the wider public. This site hosts and links academic materials that Paul has made to help other students and academics. These resources include presentations, teaching materials, code and papers. To contact Paul please email paulgarrett2016@gmail.com or find him on twitter @PaulMGarrett To see his latest work, check his Google Scholar profile or find him at Research Gate .","title":"Home"},{"location":"#home","text":"Paul Garrett is a PhD candidate at the University of Newcastle, Australia and a visiting academic at the University of Melbourne. Paul studies cognitive psychology and human decsion making from the perspectives of psychophysics, computational modelling and systems analysis. During his PhD, Paul used R, Python and Matlab to develop mathematical tools and models to diagnose changes in the processing structure and efficiency of cognitive systems (e.g., the visual system). Paul has created educational materials for undergraduate and graduate students, teaching on topics including scientific writing, statistics, coding, professional development, and cognitive psychology. Paul is passionate about scientific communication, education and mentorship. He is a strong supporter of open science (data, code and publications), and aims to make his work and resources readily available to other scientists and the wider public. This site hosts and links academic materials that Paul has made to help other students and academics. These resources include presentations, teaching materials, code and papers. To contact Paul please email paulgarrett2016@gmail.com or find him on twitter @PaulMGarrett To see his latest work, check his Google Scholar profile or find him at Research Gate .","title":" Home "},{"location":"Contactme/","text":"Contact Me To get in contact with Paul please email paulgarrett2016@gmail.com or find him on twitter @PaulMGarrett . To see his latest work, check his Google Scholar profile or find him at Research Gate . Finally, you can download a copy of Paul Garrett's CV . To contact the Newcastle Cognition Lab, headed by Prof. Scott Brown, reach us through our website or through our twitter @NewcastleCogLab .","title":"Contact Me"},{"location":"Contactme/#contact-me","text":"To get in contact with Paul please email paulgarrett2016@gmail.com or find him on twitter @PaulMGarrett . To see his latest work, check his Google Scholar profile or find him at Research Gate . Finally, you can download a copy of Paul Garrett's CV . To contact the Newcastle Cognition Lab, headed by Prof. Scott Brown, reach us through our website or through our twitter @NewcastleCogLab .","title":" Contact Me "},{"location":"EducationalMaterialsCode/","text":"Educational Materials - Coding Like many graduate researchers, I have created various teaching materials during my PhD, for example, code, presentations, and guides. Often, the many hours sunk into these works are lost to personal hard-drives or the trash-bin. Rather than waste this content, I aim to collate this material on my GitHub and have begun to provide links and descriptions for all of my content on this site. Below, I provide descriptions and downloadable links for educational content targeted at undergraduate psychology students learning to code in Matlab. Feel free to download and share: it was made to help others. Matlab Basics Matlab is a popular coding language used in a range of fields including engineering, mathematics and cognitive psychology. Leaning Matlab for a first-time programmer can be difficult; there are many new terms, concepts, and data types to learn. As part of my PhD, I had the fortune to teach Matlab to several undergraduate students and developed a series of Matlab lessons over this time targeted at cognitive psychologists. These lessons cover the following (downloadable) Matlab topics: the basics , matrix manipulation , functions, and plotting (functions and plotting tba). The code for these lessons are provided below and can be navigated through the table of contents. Starting a script % Paul Garrett % 16/04/2019 21:49:29 % Purpose: Teach basic concepts of programming in Matlab to >= 4th year % psych students. Lesson one of appx six. %% Lesson One - The Basics. % Outline % -> 1 An Introduction to Coding % - Hardcore Coding % - Variable Naming % - Commenting % -> 2 Starting Scripts % - Sections % - Clearing % - Path Setting % - Folder Making and Deletion % -> 3 Data Types and Formats % - Numeric Data % - Logical Data % - Vectors and Matricies % - Indexing and Logical Indexing % - Cells % -> 4 Loops and Nested Loops % -> 5 Logical Matrix Indexing %% An Introduction to Coding %% 1.1 Hardcore Coding %%% What is it? % This is the principle of being given a task and programming until you hit % a wall. % Once you hit a wall, you ask for help. Then an expert looks at your code % and helps with the specific problem. They may also show you how to % improve your previos code. % You then continue until you hit your next wall. Most Psych Students % (unknowingly) learn through hardcore coding. It is the fastest way to meet % your coding requirements. % %%% What are the downsides? % There are entire philosophies dedicated to different coding languages, % e.g., c++, Matlab, python, R; styles of coding e.g., obect-oriented or class % based code vs functional code; methods of best practice e.g., timing code % and quality control; that hardcore coding doesn't teach. If you are % interested in this, you will have to learn online or by asking others. % %% 1.2 Variable Naming and Commenting % Variable Nameing % Always use meaningful variable names. Always! You may revisit this code % in two weeks, or two years. Be clear and don't name things 'blah1', % 'blah2', etc. % % Types of Variable Name % Delimiter Separated Words - use spaces to separate words. Matlab will % not take DSW variable names; so let's look at alternatives % Underscore - replaces spaces with underscores. variable_name % CamelCase - variables named by capital letters e.g., C's in CamelCase % LeadingNumbers - 1Var, 2Var. Matlab does not accept leading numbers. % Alphanumeric names - e.g., Var2, Matlab does accept % % Commenting Semi-Professionally % This is a walkthrough lesson, not a proper script. Here I comment a lot. % Code comments should generally be brief, clear, and not on every % line. % % Good practice - A comment that says what the next section will do % Bad practice - Saying what each line will do % Good practice - Starting your script by saying what it's purpose is % Bad practice - Starting your script by calling a bunch of uncommented % scripts and functions. % % You will know bad commenting when you see it. When learning, feel free to % comment everything. Just keep in mind, your final analysis script will % need debugging. A clean, clear code is easy to debug. A long winded code % is very hard to debug. % And remember. In coding, it's kill or be killed. < Ctrl + c > % If you start a loop that won't end, kill it. %% 2 Starting Scripts % Start all scripts by: % i) Clearing the workspace i.e., variables. < clear > % ii) Closing all opened figures. % iii) Clearing the command line < clc > clear; close; clc; % You may wish to write code in sections. Use '%% ' to do so. %% 2.1 This is a new section % Run a section with < Crtl + enter > % Run a section and advance to the next one with < Crtl + shift + enter > % Alternately; run a line of highlighted code with < F9 > % % At the start of each section, you may want to clear all but a few % variables. KeptVar1 = 'Keep' KeptVar3 = 'Keep' KeptVar2 = 'Keep' TempVar1 = 'Drop' TempVar2 = 'Drop' TempVar3 = 'Drop' % Check your workspace. These variables will be there % %% 2.2 Clear a few select variables with clear clear TempVar1 TempVar2 TempVar3 % Now check your Workspace. Only Kept Variables will remain % %% 2.3 Or Clear all but a few variables clearvars -except KeptVar1 % Now Only KeptVar1 will remain % We will revisit clearvars in a Lesson 2 % %% 2.4 Setting Directories % Your Current Folder must contain your main script (i.e., this one) if you % wish to run the script using Sections or Run (F5) % % Check your current working directory CurrentDir = pwd % % Set your current working directory. For example, to my desktop % < cd('C:\\Users\\Paul\\Desktop') > % % If you are writing code that you want to run from the same folder, on % different computers, you may wish to run this code snippet to quickly get % the leading file path. if ispc % Check if PC or Mac userdir = getenv('USERPROFILE'); % Get Leading Home Path else userdir = getenv('HOME'); % Get Leading Root Path end %% 2.5 Make a Folder % Add a folder to the path name (+ Desktop) FolderPath = fullfile( userdir, 'Desktop', 'MyMatlabFolder' ); % Does your folder not exist < ~exist() >, then make it! if ~exist(FolderPath, 'dir') mkdir(FolderPath); end % Now, check your Desktop. There should be a 'MyMatlabFolder'! % This process is very useful for making a script and adding % folders for Figures, Saved .mat files, etc. %% 2.5 Delete a Folder % Didn't mean to make that folder? Delete it! rmdir(FolderPath) cd(CurrentDir) Types of data %% 3 Types of Data (by section) %% 3.1 Numeric Types clear; clc; close; % These are any type made from numbers Float = 2.0005; % Decimal point value Integer = int8(2.0005); % Whole digits only. Check Result. Infinity = Inf; % Positive infinity value NaNval = nan; % Not-A-Number % Use NaNs for placeholding, preallocating arrays, or, replacing data point % without changing the shape of the matrix. More on this later. % These are the primary Numeric types. To check the class of these values, % use whos whos % Notice all variables except Integer is of class Double. This means a % double-precision floating-point value, stored in 8bit format. % Real talk, you can just think of them as 'floats'. % You can change an Int to a Float through double() Integer2Float = double(Integer); % or a float to an Int via int8 Float2Integer = int8(Float); whos Logical and boolean values %% 3.2 Logical or Boolean (True/False) clear; clc; close; % Logical types can only take 2 values: 0 or 1 % They are also known as Boolean, refering to being 'True'(1) or 'False'(0) % Logical data types are very useful when you wish to index a position in a % list of numbers or identify what values are the same/different % The '==' opperator is a query statement. It asks 'is x equivilant to y' % and returns a logical result, true or false. % true and false are key words and have values of logical 1 or logical 0 true == 1 % true true == 0 % false true == true % true true == false % false false == 0 % true false == 1 % false LogicalNum = ( [1.2, 2.4, 3.6, 4.8] == [1.2, 2.0, 3.6, 4.0] ); % Returns [1,0,1,0] as only positions 1 and 3 are the same LogicalStr = strcmp( 'String1', 'String2' ); % String Compare < strcmp() > returns logical comparisons of strings. % Futher examples... 2 == 2 % true 'a' == 'a' % true 'abc' == 'abc' % [true, true, true]. This checks each character. strcmp('abc', 'abc') % true. This checks if the whole string is == any( [0,2,0] ) % true. Checks if any value is not false. all( [0,2,0] ) % false. Checks if all values are true. % The 'Not' symbolc '~' replaces True for False and False for True... true ~= false % true 2 ~= 5 % true ~strcmp('abc','def') % true ~any([0,0,0]) % true ~true == true % false % Greater than and less than symbols are an extention of boolean query. 1 > 2 % true 2 > 1 % false 1 < 2 % true 2 < 1 % false 2 < 2 % false 2 > 2 % false 2 <= 2 % true 2 >= 2 % true % As are OR queries '|', and AND queries '&' 1 > 2 | 3 > 2 % true 1 > 2 & 3 > 2 % false Vectors and matrices %% 3.3 Vectors and Matrices clear; clc; close; % Variables are any non-keyword that has been assigned a value. % For example, Var is a variable of value 2, whereas '1' is a keyword % and its value cannot be changed Var = 2; % Vectors are a series of values. Vectors may be separated by commas... Vector = [1, 2, 3, 4] % Or Spaces. Just don't mix them. Vector = [1 2 3 4] % A matrix is a series of vectors, forming columns and rows. Matrix = [1 2 3 4; 5 6 7 8; 9 10 11 12] % You can check the size of a Variable, Vector or Matrix using size RowN = size(Matrix, 1) ColumnN = size(Matrix, 2) MatSize = size(Matrix) % Finally, you can transpose a vector or matrix so the columns become rows, % or the rows become columns RowVector = [1 2 3 4] ColumnVector = RowVector' TransposedMatrix = Matrix' Indexing and logical indexing %% 3.4 Indexing and Logical Indexing clear; clc; close; % You can access the value of a Vector Position by indexing. e.g., 3rd Pos. Vector = [1 2 3 4] Position3 = Vector(3) % You can access the value of a Matrix by giving (Row, Column) indicies % e.g., Row 1, Column 3. Matrix = [1 2 3 4; 5 6 7 8; 9 10 11 12] MatPositionR1C3 = Matrix(1,3) % You can also access the value of a Matrix through a logical index % For example, I might only want odd numbers in my Matrix. % To do this I would: % i) Use mod(x,2) to identify the odd numbers, and % ii) Use logical() to convert this into a logical matrix format LogicalMatrix_Odd = mod( Matrix, 2 ) LogicalMatrix_Odd = logical( LogicalMatrix_Odd ) % 'LogicalMatrix_Even' is now a logical matrix of 1's and 0's. % Remember, this means the ones are all 'true' and zeros 'false'. % We can now use Logical Indexing to... % i) Get a vector of only the odd Values OddValues = Matrix( LogicalMatrix_Odd ) % ii) Replace even values with not-a-numbers (NaNs). Matrix_Odd = Matrix Matrix_Odd( ~OddValues ) = nan % notice the not (~) symbol % iii) Get a vector of only Even Values EvenValues = Matrix( ~LogicalMatrix_Odd ) % iv) Replace odd values with not-a-numbers(NaNs) Matrix_Even = Matrix Matrix_Even( OddValues ) = nan % But after all that, I realise. I only want a matrix with values > 6 GreaterThan6Matrix = Matrix GreaterThan6Matrix( GreaterThan6Matrix <= 6 ) = nan NaN values %% 3.4.2 Why NaNs? clearvars -except GreaterThan6Matrix Matrix % Matlab is great at producing fast, mathematical outputs % For example, to get the mean and standard deviation of each column in the % Matrix, we can perform: ColMean = mean( Matrix ) ColStd = std( Matrix ) % To get the row Mean and SD, we can use RowMean = mean( Matrix, 2) % Or < mean( Matrix' ) > RowStd = std( Matrix' ) % But to do this, matlab requires the matrix to be a set size. e.g., 3x3, % or 3x4, or 4x4 etc. If you do not have enough data to make a full matrix, % you can preallocate the required space with NaNs. % For example, we want a 4 x 4 matrix, and will fit the old Matrix data in % it. NewMatrix = nan(4, 4) NewMatrix(1:3, 1:4) = Matrix % Now calculate the column mean NanColMean = mean( NewMatrix ) % Notice this new column mean has a value of NaN! This is because you % cannot compute the mean of not-a-number. Instead, tell matlab to ignore % nan values with nanmean! NanColMean = nanmean( NewMatrix ) % Similar NaN functions % nanmax % nanmin % nanstd % nanmedian % nansum Cells and cell arrays %% 3.5 Cells and Cell Arrays clear; clc; close; % These data types are able to contain a 'cell' of any data format. % Make and access data in cells using { } brackets CellInt = {1} CellStr = {'String'} CellCell = { [2,3,4] } % Check the contents of CellCell CellCell(1) % returns the cell CellCell{1} % returns the contents of the cell % But cells can store anything! A series of cells is known as a Cell Array % Below is a cell array that contains a: % i) matrix, % ii) vector, % iii) string, % iv) another cell array, % v) logical vector CellArray = { magic(3), [1,2], 'string', ... % ... go to next line { 'A Cell', 'Another Cell!' }, logical([1,0]) }; whos Loops %% 4.1 Loops clear; close; clc; % Loops are power tools for iterating through a list of items % Note. A loop starts with 'for' and closes with 'end'. ListOfItems = 1:10 % Set the loop over the list of items % i.e., for item in the ListOfItems for item = ListOfItems % Do this thing with the item disp(item); % print item end % But maybe you have your own list of unique items, UnsortedListOfItems = [1,30,51,20] % Set the loop over the list of items for item = UnsortedListOfItems disp(item); end % Or maybe iterate through the rows in the 1st column of a Matrix Matrix = [1 2 3 4; 5 6 7 8; 9 10 11 12; 13 14 15 16]; for row = 1:size(Matrix,1) disp(Matrix(row,1)) end % You can also loop with an else or elseif statement % These are conditional loops % For example: % If ROW is 2, display the data % If ROW is 3, print ROW THREE! to the consol % Else, PRINT 'What what!' Matrix = [1 2 3 4; 5 6 7 8; 9 10 11 12; 13 14 15 16]; for row = 1:size(Matrix,1) if row == 2 disp(Matrix(row,:)); elseif row == 3 fprintf('Row Three!') else fprintf('What what!') end end Nested loops %% 4.2 Nested Loops clear; close; clc; % Nested loops are loops within loops. % For example, we might want to iterate through the rows % of each column in Matrix Matrix = [1 2 3 4; 5 6 7 8; 9 10 11 12; 13 14 15 16]; for row = 1:size(Matrix,1) for column = 1:size(Matrix,2) disp(Matrix(row,column)) end end % or the other way around for column = 1:size(Matrix,2) for row = 1:size(Matrix,1) disp(Matrix(row,column)) end end Switch loops %% 4.3 Special Loops %% 4.3. 1 Switch-Case and Switch Loops % Switch - Case - Otherwise. % The switch function is used to execute a set of conditional expressions, % when you do not wish to use boolean logic. For example: string = 'StringyStringString!'; switch string case 'String?' fprintf('Case number 1 \\n') case 'StringStringy?' fprintf('Case number 2 \\n') case 'StringyStringString!' fprintf('Case number 3 \\n') otherwise fprintf('Case not found \\n') end % Switch Loops - a special form of conditional looping, used to iterate over a string array. % The switch loop works the same way as a normal loop; provided each conditional % check were completed with string compare < strcmp(string1, string2) > strings = {'StringyStringString!', 'StringStringy?', 'String?', ... 'STRING!'}; for n = 1:numel(strings) switch strings{n} case 'String?' fprintf('Case number 1 \\n') case 'StringStringy?' fprintf('Case number 2 \\n') case 'StringyStringString!' fprintf('Case number 3 \\n') otherwise fprintf('Case not found \\n') end end While and try loops %% 4.3.2 While and Try Loops % A while loop is like a conditional for loop; but will continue until a % specific conditional statement is met. ii = 0; while ii < 5 fprintf('%d Less than 5 \\n', ii) ii = ii + 1; end % But while loops are also dangerous! They will continue into infinity. % Here is an infinite while loop, kill it with < Ctrl + c > ii = 0; while ii < 5 fprintf('%d Less than 5 \\n', ii) end %% 4.3.3 Try Statements % Another 'dangerous' function is the try statement % Try will attempt to perform a function, and, should that function crash, % move on like nothing happened. % You can build catch statements into a try function, but I strongly % recommend not using try statements in your analysis try x = unknownvariable; fprintf('This is fine. \\n') catch fprintf('Everything is on fire. \\n') end knownvariable = 10; try x = knownvariable; fprintf('This is fine. \\n') catch fprintf('Everything is on fire. \\n') end Logical matrix indexing %% 5 Logical Matrix Indexing % Nested loops are intuitive to understand, but are very slow. % If you only need to do something once; then nested loops are great. % But if you need to do something 1000 times; they suck. % Matlab was made for 'Matrix Opperations' - Matrix Laboratory % So here, we will compare Nested Loops to Logical Matrix Indexing clear; close; clc; % First, we need a timer. We can check how much time has % passed in seconds using tic and toc. Timer1 = tic; pause(1); % wait 1 seconds. TimeSinceTimer1 = toc(Timer1); % Now, let's make a BIG matrix. Matrix = magic(10000); % The challenge. Count all the numbers in the Matrix between 500 and 1000, % inclusive. % i) Using a nested for loop. LoopTimer = tic; LoopCounter = 0; for row = 1:size(Matrix,1) for column = 1:size(Matrix,2) if Matrix(row, column) >= 500 & Matrix(row, column) <= 1000 LoopCounter = LoopCounter + 1; end end end LoopTime = toc(LoopTimer); % ii) Using Logical Matrix Indexing IndexTimer = tic; IndexCounter = numel( Matrix( Matrix >= 500 & Matrix <= 1000 ) ); IndexTime = toc(IndexTimer); Ratio = LoopTime / IndexTime; fprintf('Logical Indexing was %f times faster than Looping \\n', Ratio); Exercises - The Basics %% Lesson One Exercises % Here are a few exercises you can use to help learn the principles from % Lesson One. %% Clearing Variables clear; close; clc; load('cereal.mat'); %% Remove ONLY the variable Calories from the Workspace %% Remove ALL VARIABLES EXCEPT: Sugars Cups and Fiber, from the workspace %% Remove any remaining variables from the workspace %% Change your working directory to your Desktop, Make a folder, then, delete it! %% Load In the Data Set DoggoVsPupper.mat load('DoggoVsPupper.mat') %% Find the mean Age of all Dogs % Indexing via Number Age1 = Table{:,3} Age2 = Table.Age M1 = mean( Age1 ) M2 = mean( Age2 ) %% Using Logical Indexing, get Ages for Subject 2 Subjects = Table.Subject; Ages = Table.Age; MeanAge = mean( Ages( Subjects == 2 ) ) %% Using a For Loop, % - Loop through each of the five subjects and % find the mean age Subjects = Table.Subject; Ages = Table.Age; for ii = 1:5 AGEvector = Ages( Subjects == ii ) MeanAge(ii,1) = mean( AGEvector ) end %% Use Logical Indexing to replace all incorrect RTs with a NaN value %% Calculate the mean RT, Only for subject 3 %% Calculate the mean Zscore for across all Puppers Matrix Coding Data generation %% Lesson Outline with excercises for matrix manipulation % Data generated in section 1. Sections 2:n are exercises. % Author: Paul Garrett % Date: 11/06/2019 % Make data to play with. Don't worry about how this is done, just run this % section of code and view the output clc; clear; close; rng('default'); Participants = 3; Trials = 100; Conditions = 4; ErrorRate = .2; RTmodifier = 800; % Preallocate the data matrix with NaNs (because good coding practice) Data = nan( Trials*Participants, 4 ); % Make a participant column Data(:, 1) = reshape( repmat( 1:Participants, [Trials,1]), [Trials*Participants,1]); % Make a condition column (three conditions) Data(:, 2) = reshape( repmat( 1:Conditions, [Trials/Conditions,Participants] ), [Participants*Trials,1]); % Make a random RT range with Ntrials per Participant Data(:,3:4) = rand(Trials*Participants, 2) .* repmat([RTmodifier, 1], [Trials*Participants,1] ); % Make a random Accuracy column with errors equal to the error rate Data(Data(:,4)<=.2, 4) = 0; Data(Data(:,4)~=0, 4) = 1; clearvars -except Data Participants Trials Conditions fprintf('Everyone should now have a Data matrix with four columns:\\n Column one is participant number,\\n Column two is condition number (4 conditions),\\n Column three RT, and \\n Column four is accuracy. \\n') Logical indexing & data cleaning %% Excercise Number 1.1 % Using logical indexing, get the mean accuracy across all participants % Does the error rate look like the previous ErrorRate variable? % (I hope so) MeanAcc = mean(Data(:,4)); %% Excercise Number 1.2 % Using logical indexing, get the mean RT of accuracte trials across all % participants MeanRT = mean(Data( Data(:,4)==1, 3)); %% Excercise Number 2 % Using a for loop, calculate and store the mean RT of each condition, % for each participant...but only for accurate trials! % Hint #1. If you make all of the inaccurate RTs nan values, you can use % nanmean in your calculation. % Hint #2. You can use AND e.g., &, operations to ensure the right % participant AND the right condition are being assessed. % Replace incorrect RTs with NaNs Data( Data(:,4)==0, 3) = NaN; % Preallocate your mean matrix for storage meanRTarray = nan( Participants, Conditions ); % start for loops for participants and conditions for p = 1:Participants for c = 1:Conditions CorrectRows = Data(:,1) == p & Data(:,2) == c; meanRTarray(p, c) = nanmean( Data( CorrectRows, 3) ); end end clear p c % This loop method has a few advantages % It's easy to use and easy to read, especially with this data format % But! It's not the only way we can get these means. Matrix manipulation %% Exercise Number 3 % In this section, we are going to perform the same operations as above, % but using a three dimensional array. Instead of loading all of our data, % we are going to shape this array into the following dimensions: % % Data3D = [ConditionalTrials, Condition, Participant] % % So, each row will be an RT for a conditional trial % each column will be a single condition % and the third dimension will be each participant % Please note: Everyone finds 3D arrays difficult to wrap their heads % around. Don't worry about that. Their use is not critical for your % analyses, but they are an important part of coding (especially in matlab) % and you should have some exposure to them before we move onto functions % and plotting. % Make a 3D vector of all RTs Data3D = reshape( Data(:,3), [Trials, 1, Participants]); Reshaping matrices % Function: Reshape % Reshape is used to change the size of a matrix. It takes two primary inputs % input 1: your data. % input 2: the new size dimensions. % I'll make an array at the end for you all to play with using reshape so % you can get a handle on it. Remember, the dimensions of the new array % must multiply to match the old array. % % e.g., old array = 3x3; new array = 1x9. Could never be 1x6. % % To get around this issue, you'll see me setting new matrix dimensions by % multiplying the variables we already have. % % e.g., Using Trials to specify row numbers, or later % Trials/Conditions to specify conditional row (trial) numbers % Cheack the RT for participant one is the same in the old and new matrix % There should be 74 RTs left after clearning for incorrect RTs NonNanRTs = sum( ~isnan( Data3D(:,:,1) ) ); MatchingRTs = sum( Data3D(:,:,1) == Data(Data(:,1)==1, 3) ); % Reshape your 3D matrix with conditional columns Data3D_2 = reshape( Data3D, [Trials/Conditions, Conditions, Participants] ); % Take the mean of this new 3D Array... Mean3D = nanmean( Data3D_2 ); % Mean3D will be a three dimensional array with 1 row, four columns % (conditions) and three participants along the third dimension. % This isn't quite how we usually view this kind of data. So let's change % the dimensions of the array using 'Permute' % Using permute, we can swap dimensions: % We can swap the participants (Dim 3) with the mean rows (Dim 1) MeanRT3D = permute( Mean3D , [3, 2, 1] ); % Similar functions can be done for standard deviations, medians, modes, % etc. Repeating matrices %% Exercise Number 4.1 - REPMAT % The above section was a walk through, not really an exercise. You could % run each line and see how it worked, but it didn't let you play. % So instead; use the following matrix functions to play with this next % data set. % Repmat is a function designed to repeat a matrix or vector N number of % times for you. This is useful in many instances, for example, if you % wanted to make a condition matrix for an ANOVA. Column 1 might be factor % A (high[2] vs low[1]) and column 2 might be factor B (high vs low), and % the number of trilas in each condition might be 10; % Here, we repeat [high, high] or [2, 2], for 10 rows, and one column HH = repmat([2,2], [10, 1]); HL = repmat([2,1], [10, 1]); % Fill in LH and LL variables % Then vertically concatenate these into a single matrix % You can also use repmat to make a matrix of single value % e.g., a five x five matrix of 10s TENS = repmat(10, 5); % And change change the number of rows, columns, or 3dims manually TENS2 = repmat(10, [2, 4, 5] ); % Using REPMAT, make a matrix with columns of values 1:10, % five rows down and four elements deep along the third % dimension Exercises - Matrix manipulation %% Exercise Number 4.2 - RESHAPE clear; close; clc; Trials = 20; Conditions = 4; Data = repmat(1:Conditions, [Trials, 1]); % Using RESHAPE, reshape Data into a 3D matrix with: % 20 Rows % 2 Columns % 2 Third dimensions % make sure all of the 1s and 2s are in the first element of dimension 3, % and all of the 3s and 4s are in the second element of dimension 3 % e.g., result % ans(:,:,1) = % % 1 2 % 1 2 % 1 2 % 1 2 % ... % % ans(:,:,2) = % % 3 4 % 3 4 % 3 4 % 3 4 % ... %% Exercise Number 4.3 - PERMUTE clear; close; clc; Trials = 20; Conditions = 4; Data = repmat(1:Conditions, [1, 1, Trials]); % Using PERMUTE, change the dimensions of the below matrix, so that the % third dimension becomes the first dimension. % Note: the column doesn't change. % e.g., result % ans = % % 1 2 3 4 % 1 2 3 4 % 1 2 3 4 % 1 2 3 4 % 1 2 3 4 % 1 2 3 4 % 1 2 3 4 % 1 2 3 4 % 1 2 3 4 % 1 2 3 4 % 1 2 3 4 % 1 2 3 4 % 1 2 3 4 % 1 2 3 4 % 1 2 3 4 % 1 2 3 4 % 1 2 3 4 % 1 2 3 4 % 1 2 3 4 % 1 2 3 4","title":"Edu Materials - Coding"},{"location":"EducationalMaterialsCode/#educational-materials-coding","text":"Like many graduate researchers, I have created various teaching materials during my PhD, for example, code, presentations, and guides. Often, the many hours sunk into these works are lost to personal hard-drives or the trash-bin. Rather than waste this content, I aim to collate this material on my GitHub and have begun to provide links and descriptions for all of my content on this site. Below, I provide descriptions and downloadable links for educational content targeted at undergraduate psychology students learning to code in Matlab. Feel free to download and share: it was made to help others.","title":"Educational Materials - Coding"},{"location":"EducationalMaterialsCode/#matlab-basics","text":"Matlab is a popular coding language used in a range of fields including engineering, mathematics and cognitive psychology. Leaning Matlab for a first-time programmer can be difficult; there are many new terms, concepts, and data types to learn. As part of my PhD, I had the fortune to teach Matlab to several undergraduate students and developed a series of Matlab lessons over this time targeted at cognitive psychologists. These lessons cover the following (downloadable) Matlab topics: the basics , matrix manipulation , functions, and plotting (functions and plotting tba). The code for these lessons are provided below and can be navigated through the table of contents.","title":"Matlab Basics"},{"location":"EducationalMaterialsCode/#starting-a-script","text":"% Paul Garrett % 16/04/2019 21:49:29 % Purpose: Teach basic concepts of programming in Matlab to >= 4th year % psych students. Lesson one of appx six. %% Lesson One - The Basics. % Outline % -> 1 An Introduction to Coding % - Hardcore Coding % - Variable Naming % - Commenting % -> 2 Starting Scripts % - Sections % - Clearing % - Path Setting % - Folder Making and Deletion % -> 3 Data Types and Formats % - Numeric Data % - Logical Data % - Vectors and Matricies % - Indexing and Logical Indexing % - Cells % -> 4 Loops and Nested Loops % -> 5 Logical Matrix Indexing %% An Introduction to Coding %% 1.1 Hardcore Coding %%% What is it? % This is the principle of being given a task and programming until you hit % a wall. % Once you hit a wall, you ask for help. Then an expert looks at your code % and helps with the specific problem. They may also show you how to % improve your previos code. % You then continue until you hit your next wall. Most Psych Students % (unknowingly) learn through hardcore coding. It is the fastest way to meet % your coding requirements. % %%% What are the downsides? % There are entire philosophies dedicated to different coding languages, % e.g., c++, Matlab, python, R; styles of coding e.g., obect-oriented or class % based code vs functional code; methods of best practice e.g., timing code % and quality control; that hardcore coding doesn't teach. If you are % interested in this, you will have to learn online or by asking others. % %% 1.2 Variable Naming and Commenting % Variable Nameing % Always use meaningful variable names. Always! You may revisit this code % in two weeks, or two years. Be clear and don't name things 'blah1', % 'blah2', etc. % % Types of Variable Name % Delimiter Separated Words - use spaces to separate words. Matlab will % not take DSW variable names; so let's look at alternatives % Underscore - replaces spaces with underscores. variable_name % CamelCase - variables named by capital letters e.g., C's in CamelCase % LeadingNumbers - 1Var, 2Var. Matlab does not accept leading numbers. % Alphanumeric names - e.g., Var2, Matlab does accept % % Commenting Semi-Professionally % This is a walkthrough lesson, not a proper script. Here I comment a lot. % Code comments should generally be brief, clear, and not on every % line. % % Good practice - A comment that says what the next section will do % Bad practice - Saying what each line will do % Good practice - Starting your script by saying what it's purpose is % Bad practice - Starting your script by calling a bunch of uncommented % scripts and functions. % % You will know bad commenting when you see it. When learning, feel free to % comment everything. Just keep in mind, your final analysis script will % need debugging. A clean, clear code is easy to debug. A long winded code % is very hard to debug. % And remember. In coding, it's kill or be killed. < Ctrl + c > % If you start a loop that won't end, kill it. %% 2 Starting Scripts % Start all scripts by: % i) Clearing the workspace i.e., variables. < clear > % ii) Closing all opened figures. % iii) Clearing the command line < clc > clear; close; clc; % You may wish to write code in sections. Use '%% ' to do so. %% 2.1 This is a new section % Run a section with < Crtl + enter > % Run a section and advance to the next one with < Crtl + shift + enter > % Alternately; run a line of highlighted code with < F9 > % % At the start of each section, you may want to clear all but a few % variables. KeptVar1 = 'Keep' KeptVar3 = 'Keep' KeptVar2 = 'Keep' TempVar1 = 'Drop' TempVar2 = 'Drop' TempVar3 = 'Drop' % Check your workspace. These variables will be there % %% 2.2 Clear a few select variables with clear clear TempVar1 TempVar2 TempVar3 % Now check your Workspace. Only Kept Variables will remain % %% 2.3 Or Clear all but a few variables clearvars -except KeptVar1 % Now Only KeptVar1 will remain % We will revisit clearvars in a Lesson 2 % %% 2.4 Setting Directories % Your Current Folder must contain your main script (i.e., this one) if you % wish to run the script using Sections or Run (F5) % % Check your current working directory CurrentDir = pwd % % Set your current working directory. For example, to my desktop % < cd('C:\\Users\\Paul\\Desktop') > % % If you are writing code that you want to run from the same folder, on % different computers, you may wish to run this code snippet to quickly get % the leading file path. if ispc % Check if PC or Mac userdir = getenv('USERPROFILE'); % Get Leading Home Path else userdir = getenv('HOME'); % Get Leading Root Path end %% 2.5 Make a Folder % Add a folder to the path name (+ Desktop) FolderPath = fullfile( userdir, 'Desktop', 'MyMatlabFolder' ); % Does your folder not exist < ~exist() >, then make it! if ~exist(FolderPath, 'dir') mkdir(FolderPath); end % Now, check your Desktop. There should be a 'MyMatlabFolder'! % This process is very useful for making a script and adding % folders for Figures, Saved .mat files, etc. %% 2.5 Delete a Folder % Didn't mean to make that folder? Delete it! rmdir(FolderPath) cd(CurrentDir)","title":"Starting a script"},{"location":"EducationalMaterialsCode/#types-of-data","text":"%% 3 Types of Data (by section) %% 3.1 Numeric Types clear; clc; close; % These are any type made from numbers Float = 2.0005; % Decimal point value Integer = int8(2.0005); % Whole digits only. Check Result. Infinity = Inf; % Positive infinity value NaNval = nan; % Not-A-Number % Use NaNs for placeholding, preallocating arrays, or, replacing data point % without changing the shape of the matrix. More on this later. % These are the primary Numeric types. To check the class of these values, % use whos whos % Notice all variables except Integer is of class Double. This means a % double-precision floating-point value, stored in 8bit format. % Real talk, you can just think of them as 'floats'. % You can change an Int to a Float through double() Integer2Float = double(Integer); % or a float to an Int via int8 Float2Integer = int8(Float); whos","title":"Types of data"},{"location":"EducationalMaterialsCode/#logical-and-boolean-values","text":"%% 3.2 Logical or Boolean (True/False) clear; clc; close; % Logical types can only take 2 values: 0 or 1 % They are also known as Boolean, refering to being 'True'(1) or 'False'(0) % Logical data types are very useful when you wish to index a position in a % list of numbers or identify what values are the same/different % The '==' opperator is a query statement. It asks 'is x equivilant to y' % and returns a logical result, true or false. % true and false are key words and have values of logical 1 or logical 0 true == 1 % true true == 0 % false true == true % true true == false % false false == 0 % true false == 1 % false LogicalNum = ( [1.2, 2.4, 3.6, 4.8] == [1.2, 2.0, 3.6, 4.0] ); % Returns [1,0,1,0] as only positions 1 and 3 are the same LogicalStr = strcmp( 'String1', 'String2' ); % String Compare < strcmp() > returns logical comparisons of strings. % Futher examples... 2 == 2 % true 'a' == 'a' % true 'abc' == 'abc' % [true, true, true]. This checks each character. strcmp('abc', 'abc') % true. This checks if the whole string is == any( [0,2,0] ) % true. Checks if any value is not false. all( [0,2,0] ) % false. Checks if all values are true. % The 'Not' symbolc '~' replaces True for False and False for True... true ~= false % true 2 ~= 5 % true ~strcmp('abc','def') % true ~any([0,0,0]) % true ~true == true % false % Greater than and less than symbols are an extention of boolean query. 1 > 2 % true 2 > 1 % false 1 < 2 % true 2 < 1 % false 2 < 2 % false 2 > 2 % false 2 <= 2 % true 2 >= 2 % true % As are OR queries '|', and AND queries '&' 1 > 2 | 3 > 2 % true 1 > 2 & 3 > 2 % false","title":"Logical and boolean values"},{"location":"EducationalMaterialsCode/#vectors-and-matrices","text":"%% 3.3 Vectors and Matrices clear; clc; close; % Variables are any non-keyword that has been assigned a value. % For example, Var is a variable of value 2, whereas '1' is a keyword % and its value cannot be changed Var = 2; % Vectors are a series of values. Vectors may be separated by commas... Vector = [1, 2, 3, 4] % Or Spaces. Just don't mix them. Vector = [1 2 3 4] % A matrix is a series of vectors, forming columns and rows. Matrix = [1 2 3 4; 5 6 7 8; 9 10 11 12] % You can check the size of a Variable, Vector or Matrix using size RowN = size(Matrix, 1) ColumnN = size(Matrix, 2) MatSize = size(Matrix) % Finally, you can transpose a vector or matrix so the columns become rows, % or the rows become columns RowVector = [1 2 3 4] ColumnVector = RowVector' TransposedMatrix = Matrix'","title":"Vectors and matrices"},{"location":"EducationalMaterialsCode/#indexing-and-logical-indexing","text":"%% 3.4 Indexing and Logical Indexing clear; clc; close; % You can access the value of a Vector Position by indexing. e.g., 3rd Pos. Vector = [1 2 3 4] Position3 = Vector(3) % You can access the value of a Matrix by giving (Row, Column) indicies % e.g., Row 1, Column 3. Matrix = [1 2 3 4; 5 6 7 8; 9 10 11 12] MatPositionR1C3 = Matrix(1,3) % You can also access the value of a Matrix through a logical index % For example, I might only want odd numbers in my Matrix. % To do this I would: % i) Use mod(x,2) to identify the odd numbers, and % ii) Use logical() to convert this into a logical matrix format LogicalMatrix_Odd = mod( Matrix, 2 ) LogicalMatrix_Odd = logical( LogicalMatrix_Odd ) % 'LogicalMatrix_Even' is now a logical matrix of 1's and 0's. % Remember, this means the ones are all 'true' and zeros 'false'. % We can now use Logical Indexing to... % i) Get a vector of only the odd Values OddValues = Matrix( LogicalMatrix_Odd ) % ii) Replace even values with not-a-numbers (NaNs). Matrix_Odd = Matrix Matrix_Odd( ~OddValues ) = nan % notice the not (~) symbol % iii) Get a vector of only Even Values EvenValues = Matrix( ~LogicalMatrix_Odd ) % iv) Replace odd values with not-a-numbers(NaNs) Matrix_Even = Matrix Matrix_Even( OddValues ) = nan % But after all that, I realise. I only want a matrix with values > 6 GreaterThan6Matrix = Matrix GreaterThan6Matrix( GreaterThan6Matrix <= 6 ) = nan","title":"Indexing and logical indexing"},{"location":"EducationalMaterialsCode/#nan-values","text":"%% 3.4.2 Why NaNs? clearvars -except GreaterThan6Matrix Matrix % Matlab is great at producing fast, mathematical outputs % For example, to get the mean and standard deviation of each column in the % Matrix, we can perform: ColMean = mean( Matrix ) ColStd = std( Matrix ) % To get the row Mean and SD, we can use RowMean = mean( Matrix, 2) % Or < mean( Matrix' ) > RowStd = std( Matrix' ) % But to do this, matlab requires the matrix to be a set size. e.g., 3x3, % or 3x4, or 4x4 etc. If you do not have enough data to make a full matrix, % you can preallocate the required space with NaNs. % For example, we want a 4 x 4 matrix, and will fit the old Matrix data in % it. NewMatrix = nan(4, 4) NewMatrix(1:3, 1:4) = Matrix % Now calculate the column mean NanColMean = mean( NewMatrix ) % Notice this new column mean has a value of NaN! This is because you % cannot compute the mean of not-a-number. Instead, tell matlab to ignore % nan values with nanmean! NanColMean = nanmean( NewMatrix ) % Similar NaN functions % nanmax % nanmin % nanstd % nanmedian % nansum","title":"NaN values"},{"location":"EducationalMaterialsCode/#cells-and-cell-arrays","text":"%% 3.5 Cells and Cell Arrays clear; clc; close; % These data types are able to contain a 'cell' of any data format. % Make and access data in cells using { } brackets CellInt = {1} CellStr = {'String'} CellCell = { [2,3,4] } % Check the contents of CellCell CellCell(1) % returns the cell CellCell{1} % returns the contents of the cell % But cells can store anything! A series of cells is known as a Cell Array % Below is a cell array that contains a: % i) matrix, % ii) vector, % iii) string, % iv) another cell array, % v) logical vector CellArray = { magic(3), [1,2], 'string', ... % ... go to next line { 'A Cell', 'Another Cell!' }, logical([1,0]) }; whos","title":"Cells and cell arrays"},{"location":"EducationalMaterialsCode/#loops","text":"%% 4.1 Loops clear; close; clc; % Loops are power tools for iterating through a list of items % Note. A loop starts with 'for' and closes with 'end'. ListOfItems = 1:10 % Set the loop over the list of items % i.e., for item in the ListOfItems for item = ListOfItems % Do this thing with the item disp(item); % print item end % But maybe you have your own list of unique items, UnsortedListOfItems = [1,30,51,20] % Set the loop over the list of items for item = UnsortedListOfItems disp(item); end % Or maybe iterate through the rows in the 1st column of a Matrix Matrix = [1 2 3 4; 5 6 7 8; 9 10 11 12; 13 14 15 16]; for row = 1:size(Matrix,1) disp(Matrix(row,1)) end % You can also loop with an else or elseif statement % These are conditional loops % For example: % If ROW is 2, display the data % If ROW is 3, print ROW THREE! to the consol % Else, PRINT 'What what!' Matrix = [1 2 3 4; 5 6 7 8; 9 10 11 12; 13 14 15 16]; for row = 1:size(Matrix,1) if row == 2 disp(Matrix(row,:)); elseif row == 3 fprintf('Row Three!') else fprintf('What what!') end end","title":"Loops"},{"location":"EducationalMaterialsCode/#nested-loops","text":"%% 4.2 Nested Loops clear; close; clc; % Nested loops are loops within loops. % For example, we might want to iterate through the rows % of each column in Matrix Matrix = [1 2 3 4; 5 6 7 8; 9 10 11 12; 13 14 15 16]; for row = 1:size(Matrix,1) for column = 1:size(Matrix,2) disp(Matrix(row,column)) end end % or the other way around for column = 1:size(Matrix,2) for row = 1:size(Matrix,1) disp(Matrix(row,column)) end end","title":"Nested loops"},{"location":"EducationalMaterialsCode/#switch-loops","text":"%% 4.3 Special Loops %% 4.3. 1 Switch-Case and Switch Loops % Switch - Case - Otherwise. % The switch function is used to execute a set of conditional expressions, % when you do not wish to use boolean logic. For example: string = 'StringyStringString!'; switch string case 'String?' fprintf('Case number 1 \\n') case 'StringStringy?' fprintf('Case number 2 \\n') case 'StringyStringString!' fprintf('Case number 3 \\n') otherwise fprintf('Case not found \\n') end % Switch Loops - a special form of conditional looping, used to iterate over a string array. % The switch loop works the same way as a normal loop; provided each conditional % check were completed with string compare < strcmp(string1, string2) > strings = {'StringyStringString!', 'StringStringy?', 'String?', ... 'STRING!'}; for n = 1:numel(strings) switch strings{n} case 'String?' fprintf('Case number 1 \\n') case 'StringStringy?' fprintf('Case number 2 \\n') case 'StringyStringString!' fprintf('Case number 3 \\n') otherwise fprintf('Case not found \\n') end end","title":"Switch loops"},{"location":"EducationalMaterialsCode/#while-and-try-loops","text":"%% 4.3.2 While and Try Loops % A while loop is like a conditional for loop; but will continue until a % specific conditional statement is met. ii = 0; while ii < 5 fprintf('%d Less than 5 \\n', ii) ii = ii + 1; end % But while loops are also dangerous! They will continue into infinity. % Here is an infinite while loop, kill it with < Ctrl + c > ii = 0; while ii < 5 fprintf('%d Less than 5 \\n', ii) end %% 4.3.3 Try Statements % Another 'dangerous' function is the try statement % Try will attempt to perform a function, and, should that function crash, % move on like nothing happened. % You can build catch statements into a try function, but I strongly % recommend not using try statements in your analysis try x = unknownvariable; fprintf('This is fine. \\n') catch fprintf('Everything is on fire. \\n') end knownvariable = 10; try x = knownvariable; fprintf('This is fine. \\n') catch fprintf('Everything is on fire. \\n') end","title":"While and try loops"},{"location":"EducationalMaterialsCode/#logical-matrix-indexing","text":"%% 5 Logical Matrix Indexing % Nested loops are intuitive to understand, but are very slow. % If you only need to do something once; then nested loops are great. % But if you need to do something 1000 times; they suck. % Matlab was made for 'Matrix Opperations' - Matrix Laboratory % So here, we will compare Nested Loops to Logical Matrix Indexing clear; close; clc; % First, we need a timer. We can check how much time has % passed in seconds using tic and toc. Timer1 = tic; pause(1); % wait 1 seconds. TimeSinceTimer1 = toc(Timer1); % Now, let's make a BIG matrix. Matrix = magic(10000); % The challenge. Count all the numbers in the Matrix between 500 and 1000, % inclusive. % i) Using a nested for loop. LoopTimer = tic; LoopCounter = 0; for row = 1:size(Matrix,1) for column = 1:size(Matrix,2) if Matrix(row, column) >= 500 & Matrix(row, column) <= 1000 LoopCounter = LoopCounter + 1; end end end LoopTime = toc(LoopTimer); % ii) Using Logical Matrix Indexing IndexTimer = tic; IndexCounter = numel( Matrix( Matrix >= 500 & Matrix <= 1000 ) ); IndexTime = toc(IndexTimer); Ratio = LoopTime / IndexTime; fprintf('Logical Indexing was %f times faster than Looping \\n', Ratio);","title":"Logical matrix indexing"},{"location":"EducationalMaterialsCode/#exercises-the-basics","text":"%% Lesson One Exercises % Here are a few exercises you can use to help learn the principles from % Lesson One. %% Clearing Variables clear; close; clc; load('cereal.mat'); %% Remove ONLY the variable Calories from the Workspace %% Remove ALL VARIABLES EXCEPT: Sugars Cups and Fiber, from the workspace %% Remove any remaining variables from the workspace %% Change your working directory to your Desktop, Make a folder, then, delete it! %% Load In the Data Set DoggoVsPupper.mat load('DoggoVsPupper.mat') %% Find the mean Age of all Dogs % Indexing via Number Age1 = Table{:,3} Age2 = Table.Age M1 = mean( Age1 ) M2 = mean( Age2 ) %% Using Logical Indexing, get Ages for Subject 2 Subjects = Table.Subject; Ages = Table.Age; MeanAge = mean( Ages( Subjects == 2 ) ) %% Using a For Loop, % - Loop through each of the five subjects and % find the mean age Subjects = Table.Subject; Ages = Table.Age; for ii = 1:5 AGEvector = Ages( Subjects == ii ) MeanAge(ii,1) = mean( AGEvector ) end %% Use Logical Indexing to replace all incorrect RTs with a NaN value %% Calculate the mean RT, Only for subject 3 %% Calculate the mean Zscore for across all Puppers","title":"Exercises - The Basics"},{"location":"EducationalMaterialsCode/#matrix-coding","text":"","title":"Matrix Coding"},{"location":"EducationalMaterialsCode/#data-generation","text":"%% Lesson Outline with excercises for matrix manipulation % Data generated in section 1. Sections 2:n are exercises. % Author: Paul Garrett % Date: 11/06/2019 % Make data to play with. Don't worry about how this is done, just run this % section of code and view the output clc; clear; close; rng('default'); Participants = 3; Trials = 100; Conditions = 4; ErrorRate = .2; RTmodifier = 800; % Preallocate the data matrix with NaNs (because good coding practice) Data = nan( Trials*Participants, 4 ); % Make a participant column Data(:, 1) = reshape( repmat( 1:Participants, [Trials,1]), [Trials*Participants,1]); % Make a condition column (three conditions) Data(:, 2) = reshape( repmat( 1:Conditions, [Trials/Conditions,Participants] ), [Participants*Trials,1]); % Make a random RT range with Ntrials per Participant Data(:,3:4) = rand(Trials*Participants, 2) .* repmat([RTmodifier, 1], [Trials*Participants,1] ); % Make a random Accuracy column with errors equal to the error rate Data(Data(:,4)<=.2, 4) = 0; Data(Data(:,4)~=0, 4) = 1; clearvars -except Data Participants Trials Conditions fprintf('Everyone should now have a Data matrix with four columns:\\n Column one is participant number,\\n Column two is condition number (4 conditions),\\n Column three RT, and \\n Column four is accuracy. \\n')","title":"Data generation"},{"location":"EducationalMaterialsCode/#logical-indexing-data-cleaning","text":"%% Excercise Number 1.1 % Using logical indexing, get the mean accuracy across all participants % Does the error rate look like the previous ErrorRate variable? % (I hope so) MeanAcc = mean(Data(:,4)); %% Excercise Number 1.2 % Using logical indexing, get the mean RT of accuracte trials across all % participants MeanRT = mean(Data( Data(:,4)==1, 3)); %% Excercise Number 2 % Using a for loop, calculate and store the mean RT of each condition, % for each participant...but only for accurate trials! % Hint #1. If you make all of the inaccurate RTs nan values, you can use % nanmean in your calculation. % Hint #2. You can use AND e.g., &, operations to ensure the right % participant AND the right condition are being assessed. % Replace incorrect RTs with NaNs Data( Data(:,4)==0, 3) = NaN; % Preallocate your mean matrix for storage meanRTarray = nan( Participants, Conditions ); % start for loops for participants and conditions for p = 1:Participants for c = 1:Conditions CorrectRows = Data(:,1) == p & Data(:,2) == c; meanRTarray(p, c) = nanmean( Data( CorrectRows, 3) ); end end clear p c % This loop method has a few advantages % It's easy to use and easy to read, especially with this data format % But! It's not the only way we can get these means.","title":"Logical indexing &amp; data cleaning"},{"location":"EducationalMaterialsCode/#matrix-manipulation","text":"%% Exercise Number 3 % In this section, we are going to perform the same operations as above, % but using a three dimensional array. Instead of loading all of our data, % we are going to shape this array into the following dimensions: % % Data3D = [ConditionalTrials, Condition, Participant] % % So, each row will be an RT for a conditional trial % each column will be a single condition % and the third dimension will be each participant % Please note: Everyone finds 3D arrays difficult to wrap their heads % around. Don't worry about that. Their use is not critical for your % analyses, but they are an important part of coding (especially in matlab) % and you should have some exposure to them before we move onto functions % and plotting. % Make a 3D vector of all RTs Data3D = reshape( Data(:,3), [Trials, 1, Participants]);","title":"Matrix manipulation"},{"location":"EducationalMaterialsCode/#reshaping-matrices","text":"% Function: Reshape % Reshape is used to change the size of a matrix. It takes two primary inputs % input 1: your data. % input 2: the new size dimensions. % I'll make an array at the end for you all to play with using reshape so % you can get a handle on it. Remember, the dimensions of the new array % must multiply to match the old array. % % e.g., old array = 3x3; new array = 1x9. Could never be 1x6. % % To get around this issue, you'll see me setting new matrix dimensions by % multiplying the variables we already have. % % e.g., Using Trials to specify row numbers, or later % Trials/Conditions to specify conditional row (trial) numbers % Cheack the RT for participant one is the same in the old and new matrix % There should be 74 RTs left after clearning for incorrect RTs NonNanRTs = sum( ~isnan( Data3D(:,:,1) ) ); MatchingRTs = sum( Data3D(:,:,1) == Data(Data(:,1)==1, 3) ); % Reshape your 3D matrix with conditional columns Data3D_2 = reshape( Data3D, [Trials/Conditions, Conditions, Participants] ); % Take the mean of this new 3D Array... Mean3D = nanmean( Data3D_2 ); % Mean3D will be a three dimensional array with 1 row, four columns % (conditions) and three participants along the third dimension. % This isn't quite how we usually view this kind of data. So let's change % the dimensions of the array using 'Permute' % Using permute, we can swap dimensions: % We can swap the participants (Dim 3) with the mean rows (Dim 1) MeanRT3D = permute( Mean3D , [3, 2, 1] ); % Similar functions can be done for standard deviations, medians, modes, % etc.","title":"Reshaping matrices"},{"location":"EducationalMaterialsCode/#repeating-matrices","text":"%% Exercise Number 4.1 - REPMAT % The above section was a walk through, not really an exercise. You could % run each line and see how it worked, but it didn't let you play. % So instead; use the following matrix functions to play with this next % data set. % Repmat is a function designed to repeat a matrix or vector N number of % times for you. This is useful in many instances, for example, if you % wanted to make a condition matrix for an ANOVA. Column 1 might be factor % A (high[2] vs low[1]) and column 2 might be factor B (high vs low), and % the number of trilas in each condition might be 10; % Here, we repeat [high, high] or [2, 2], for 10 rows, and one column HH = repmat([2,2], [10, 1]); HL = repmat([2,1], [10, 1]); % Fill in LH and LL variables % Then vertically concatenate these into a single matrix % You can also use repmat to make a matrix of single value % e.g., a five x five matrix of 10s TENS = repmat(10, 5); % And change change the number of rows, columns, or 3dims manually TENS2 = repmat(10, [2, 4, 5] ); % Using REPMAT, make a matrix with columns of values 1:10, % five rows down and four elements deep along the third % dimension","title":"Repeating matrices"},{"location":"EducationalMaterialsCode/#exercises-matrix-manipulation","text":"%% Exercise Number 4.2 - RESHAPE clear; close; clc; Trials = 20; Conditions = 4; Data = repmat(1:Conditions, [Trials, 1]); % Using RESHAPE, reshape Data into a 3D matrix with: % 20 Rows % 2 Columns % 2 Third dimensions % make sure all of the 1s and 2s are in the first element of dimension 3, % and all of the 3s and 4s are in the second element of dimension 3 % e.g., result % ans(:,:,1) = % % 1 2 % 1 2 % 1 2 % 1 2 % ... % % ans(:,:,2) = % % 3 4 % 3 4 % 3 4 % 3 4 % ... %% Exercise Number 4.3 - PERMUTE clear; close; clc; Trials = 20; Conditions = 4; Data = repmat(1:Conditions, [1, 1, Trials]); % Using PERMUTE, change the dimensions of the below matrix, so that the % third dimension becomes the first dimension. % Note: the column doesn't change. % e.g., result % ans = % % 1 2 3 4 % 1 2 3 4 % 1 2 3 4 % 1 2 3 4 % 1 2 3 4 % 1 2 3 4 % 1 2 3 4 % 1 2 3 4 % 1 2 3 4 % 1 2 3 4 % 1 2 3 4 % 1 2 3 4 % 1 2 3 4 % 1 2 3 4 % 1 2 3 4 % 1 2 3 4 % 1 2 3 4 % 1 2 3 4 % 1 2 3 4 % 1 2 3 4","title":"Exercises - Matrix manipulation"},{"location":"EducationalMaterialsWriting/","text":"Educational Materials - Scientific Writing Like many graduate researchers, I have created various teaching materials during my PhD, for example, code, presentations, and guides. Often, the many hours sunk into these works are lost to personal hard-drives or the trash-bin. Rather than waste this content, I aim to collate this material on my GitHub and have begun to provide links and descriptions for all of my content on this site. Below, I provide descriptions and downloadable links for educational content targeted at undergraduate psychology students. Topics include scientific writing, statistics and making APA figures. A separate tab has been dedicated to resources on learning to code in Matlab. Feel free to download and share this content: it was made to help others. Statistics Statistics can be difficult at the best of times. Often students progress quickly though statistical content with the aim of passing a single exam (e.g., run a t-test, ANOVA or ANCOVA). These tests are made simple in statistical programs such as SPSS, JASP or JAMOVI; however, students regularly forget the basics. For this reason, I created a simple crash course on understanding elements of basic statistics. This crash-course has helped numerous undergraduate psychology students quickly catch-up on simple (but critical) concepts in statistics, from students in first up to fourth year. Follow these links for a quick crash course on basic stats , and an excel walk-through on central measures ; hopefully, you'll know it all anyway! Report writing Report writing is a critical skill for undergraduate scientists. Although these skills are needed to excel at many undergraduate courses, these skills are often not taught to students. During my time as a 2000/3000 level Psychology tutor, and as part of my private tutoring work in scientific methodology and design, I created a simple and clear walk-through. This tutorial covers the scientific method, report writing (what to include, where and why!), understanding and reporting statistical results (including interactions within two-way ANOVAs and simple effects), and an overview on how to write better (at an undergraduate level). Importantly, clear examples are given at every step so you can make the most of each section. Download this tutorial on Report writing with Paul . During my time as a tutor, I often found that student's lacked an example of what they should aim to achieve and what markers might be looking for. Of course, some excellent students will go above-and-beyond to create a perfect report (and kudos to them!) But a benchmark example should be always be clearly established for students aiming to pass or receive a credit. This is especially important for students writing their very first 500 word report. To this end, I created a small example report with comments and feedback. The report isn't perfect, however, compliments the structure detailed in the above report-writing tutorial. If you are new to scientific report writing and want a simple example to work towards, this example report was made for you. APA figures Making clear, accurate APA style figures is an important skill in psychology. Although graduate students (PhD, Masters) often plot figures in programs such as Python, Matlab or R, undergraduate students often plot such figures in Excel. APA figures are simple to make, however, require several steps in Excel to get right. To make this process easier, students may download this simple APA Plotting Walk-Though for creating APA-style figures in Excel (example provided below). Writing Better Anyone who attempts or completes a PhD or Masters will tell you \"writing is hard\". In my experience, writing is the skill that graduate students require the most help with, after all, we only get better at writing through practice and feedback. Although there is no substitute for the process of writing-editing-feedback-writing; there are some helpful hints I have been taught and discovered along the way. These tips are targeted at those individuals who aim to complete a large body of work: honours, masters or PhD theses, academic papers or conference proceedings; however, these tips may also help undergraduates students or any intrepid writer. Writing \u2260 Editing - Writing a clear and flowing document is hard work. It takes time, practice, and lots of editing; but writing is not editing. Writing is where you let ideas flow (or vomit in messy succession) onto the page. When writing, you don't worry about commas, or going back to fix a capital letter, or if the paragraph or sentence is too long. All you need to do is get the words out so that you have something to structure when you edit. Editing is the process of taking your text, and refining it to fit the document structure (article, manuscript, report, letter), join paragraphs and create flow, to make sentences concise, and to ensure you have hit all of your key requirements (e.g., ensuring your research article as a clear aim!) During my undergraduate degree, the difference between writing and editing was never stressed. I aimed to perform both processes simultaneous. Although my work received adequate marks, I never appreciated how a well written assignment shifted my a grade from a distinction to a high-distinction. The truth is, the easier your content is to read, the less-work your marker must put in, and the more time they have to look and give you the marks your work deserves. This goes triple for anyone dealing with reviewers when publishing scientific articles! Write a little, regularly - If you need to write a large body of text, or indeed, many large bodies of text, it is important to stay in practice. Writing is a skill, and just like other cognitive skills (e.g., mathematics), if you don't use it you'll lose it. Set a time. At least once a week, set some time aside to write (not edit!) early in the morning, even for half-an-hour. This time must be early, otherwise, you will find an excuse to put it off (e.g., meetings run overtime, other work comes up, coffee runs must be made). Use a timer. It is hard to focus on writing for an extended time, so don't. Write with a timed technique, such as the Pomodoro 'tomato timer' technique. Under the Pomodoro, you write uninterrupted for 25 minutes, break for five, and write for another 25. Easy, simple and very effective. \u2018Park on the Hill\u2019 - The hardest part about writing is starting. Make it easier by parking on the hill. At the end of each writing session, don't simply stop at the end of a paragraph (tempting thought it is). Help future you by writing some dot points on what your going to do next, so when you return, your job is easier and you are no longer facing the blank empty void that is your next paragraph. Be accountable - It's easy to say \"I'll do it next week\" or \"It'll still be there tomorrow\". We all make excuses for our lack of writing or find other 'useful' activities to blame for our procrastination. Stop this by being accountable. If you are a student, make a writing group. It can just be you and a friend (it's how I got through my PhD Thesis - Thank Gabe!). Meet at a regular time and place, set small achievable goals at the start of each session (can just be an hour), and write. Make sure your friend can see whether you are meeting your goals, and work towards a reward (a coffee, a game, an hour's free time) for meeting each goal. If you are lucky enough to have a supervisor, make sure they see these goals and help keep you accountable. Invest in grammar - You probably cannot tell by my writing, but learning about grammar is a favourite past-time of mine. It started as a method of procrastination, but quickly developed into a skill I could use to improve my writing. Ever wonder \"how do I use a semi-colon?\", or \"what's an Oxford comma?\", what about \"what's the difference between e.g., and i.e.,?\", \"what is a dash, en-dash and em-dash?\" and \"when do I use parentheses?\" These are questions I had as I wrote. Learning the answers was fascinating and helped my text become much clearer. Be Brief: Rule of 3s - If you don't have a rule of thumb when writing scientific reports, use the rule of 3s. Put simply, this states there must be \u2265 3 words to a sentence, \u2265 3 sentences to a paragraph, and \u2265 3 paragraphs to a page. This rule is very harsh and is intended to enforce structure to your sentences and paragraphs, especially when used in tandem with the next point: topic and linkage sentences. Topic & Linkage Sentences - Your first sentence should tell the reader the primary message of the paragraph. Your last sentence should directly lead onto the topic sentence of the next paragraph. You middle sentence(s) are the 'body' of the argument raised in the first sentence. This structure enforces flow within and between paragraphs. Topic and linkage sentences are hard to achieve when you start (often you fix these when editing), but eventually, the use of this structure makes you a better writer and helps you present clear, strong arguments --- because every paragraph will clearly relate to one-another. Active voice - Use active voice: [e.g., Littel et al (2018) found that X impacts Y.], not passive voice [e.g., X has been shown to impact Y (Little et al, 2018)]. Active voice is engaging, it makes you want to read on! Passive voice is dispassionate and rather dull -- not something you want for your 5000 word essay! Use sub-headings - Sub-headings are a roadmap to your work, especially in scientific writing. If you are writing anything over 1500 words; you should probably have sub-headings. Not only do sub-headings give your reader a chance to pause and collect their thoughts, they provide a clear description of how your argument was made and where your marks should be allocated. Anything to help your marker give 'more marks' or better understand your argument is a good thing. These tips are small and simple, however, I have no doubt that they helped me write my 50,000 word PhD thesis and three academic articles in the space of six months. I endorse each of these tips fully, and hope they help other writers put words to page and maybe even learn to enjoy the writing process.","title":"Edu Materials - Writing"},{"location":"EducationalMaterialsWriting/#educational-materials-scientific-writing","text":"Like many graduate researchers, I have created various teaching materials during my PhD, for example, code, presentations, and guides. Often, the many hours sunk into these works are lost to personal hard-drives or the trash-bin. Rather than waste this content, I aim to collate this material on my GitHub and have begun to provide links and descriptions for all of my content on this site. Below, I provide descriptions and downloadable links for educational content targeted at undergraduate psychology students. Topics include scientific writing, statistics and making APA figures. A separate tab has been dedicated to resources on learning to code in Matlab. Feel free to download and share this content: it was made to help others.","title":"Educational Materials  - Scientific Writing"},{"location":"EducationalMaterialsWriting/#statistics","text":"Statistics can be difficult at the best of times. Often students progress quickly though statistical content with the aim of passing a single exam (e.g., run a t-test, ANOVA or ANCOVA). These tests are made simple in statistical programs such as SPSS, JASP or JAMOVI; however, students regularly forget the basics. For this reason, I created a simple crash course on understanding elements of basic statistics. This crash-course has helped numerous undergraduate psychology students quickly catch-up on simple (but critical) concepts in statistics, from students in first up to fourth year. Follow these links for a quick crash course on basic stats , and an excel walk-through on central measures ; hopefully, you'll know it all anyway!","title":"Statistics"},{"location":"EducationalMaterialsWriting/#report-writing","text":"Report writing is a critical skill for undergraduate scientists. Although these skills are needed to excel at many undergraduate courses, these skills are often not taught to students. During my time as a 2000/3000 level Psychology tutor, and as part of my private tutoring work in scientific methodology and design, I created a simple and clear walk-through. This tutorial covers the scientific method, report writing (what to include, where and why!), understanding and reporting statistical results (including interactions within two-way ANOVAs and simple effects), and an overview on how to write better (at an undergraduate level). Importantly, clear examples are given at every step so you can make the most of each section. Download this tutorial on Report writing with Paul . During my time as a tutor, I often found that student's lacked an example of what they should aim to achieve and what markers might be looking for. Of course, some excellent students will go above-and-beyond to create a perfect report (and kudos to them!) But a benchmark example should be always be clearly established for students aiming to pass or receive a credit. This is especially important for students writing their very first 500 word report. To this end, I created a small example report with comments and feedback. The report isn't perfect, however, compliments the structure detailed in the above report-writing tutorial. If you are new to scientific report writing and want a simple example to work towards, this example report was made for you.","title":"Report writing"},{"location":"EducationalMaterialsWriting/#apa-figures","text":"Making clear, accurate APA style figures is an important skill in psychology. Although graduate students (PhD, Masters) often plot figures in programs such as Python, Matlab or R, undergraduate students often plot such figures in Excel. APA figures are simple to make, however, require several steps in Excel to get right. To make this process easier, students may download this simple APA Plotting Walk-Though for creating APA-style figures in Excel (example provided below).","title":"APA figures"},{"location":"EducationalMaterialsWriting/#writing-better","text":"Anyone who attempts or completes a PhD or Masters will tell you \"writing is hard\". In my experience, writing is the skill that graduate students require the most help with, after all, we only get better at writing through practice and feedback. Although there is no substitute for the process of writing-editing-feedback-writing; there are some helpful hints I have been taught and discovered along the way. These tips are targeted at those individuals who aim to complete a large body of work: honours, masters or PhD theses, academic papers or conference proceedings; however, these tips may also help undergraduates students or any intrepid writer. Writing \u2260 Editing - Writing a clear and flowing document is hard work. It takes time, practice, and lots of editing; but writing is not editing. Writing is where you let ideas flow (or vomit in messy succession) onto the page. When writing, you don't worry about commas, or going back to fix a capital letter, or if the paragraph or sentence is too long. All you need to do is get the words out so that you have something to structure when you edit. Editing is the process of taking your text, and refining it to fit the document structure (article, manuscript, report, letter), join paragraphs and create flow, to make sentences concise, and to ensure you have hit all of your key requirements (e.g., ensuring your research article as a clear aim!) During my undergraduate degree, the difference between writing and editing was never stressed. I aimed to perform both processes simultaneous. Although my work received adequate marks, I never appreciated how a well written assignment shifted my a grade from a distinction to a high-distinction. The truth is, the easier your content is to read, the less-work your marker must put in, and the more time they have to look and give you the marks your work deserves. This goes triple for anyone dealing with reviewers when publishing scientific articles! Write a little, regularly - If you need to write a large body of text, or indeed, many large bodies of text, it is important to stay in practice. Writing is a skill, and just like other cognitive skills (e.g., mathematics), if you don't use it you'll lose it. Set a time. At least once a week, set some time aside to write (not edit!) early in the morning, even for half-an-hour. This time must be early, otherwise, you will find an excuse to put it off (e.g., meetings run overtime, other work comes up, coffee runs must be made). Use a timer. It is hard to focus on writing for an extended time, so don't. Write with a timed technique, such as the Pomodoro 'tomato timer' technique. Under the Pomodoro, you write uninterrupted for 25 minutes, break for five, and write for another 25. Easy, simple and very effective. \u2018Park on the Hill\u2019 - The hardest part about writing is starting. Make it easier by parking on the hill. At the end of each writing session, don't simply stop at the end of a paragraph (tempting thought it is). Help future you by writing some dot points on what your going to do next, so when you return, your job is easier and you are no longer facing the blank empty void that is your next paragraph. Be accountable - It's easy to say \"I'll do it next week\" or \"It'll still be there tomorrow\". We all make excuses for our lack of writing or find other 'useful' activities to blame for our procrastination. Stop this by being accountable. If you are a student, make a writing group. It can just be you and a friend (it's how I got through my PhD Thesis - Thank Gabe!). Meet at a regular time and place, set small achievable goals at the start of each session (can just be an hour), and write. Make sure your friend can see whether you are meeting your goals, and work towards a reward (a coffee, a game, an hour's free time) for meeting each goal. If you are lucky enough to have a supervisor, make sure they see these goals and help keep you accountable. Invest in grammar - You probably cannot tell by my writing, but learning about grammar is a favourite past-time of mine. It started as a method of procrastination, but quickly developed into a skill I could use to improve my writing. Ever wonder \"how do I use a semi-colon?\", or \"what's an Oxford comma?\", what about \"what's the difference between e.g., and i.e.,?\", \"what is a dash, en-dash and em-dash?\" and \"when do I use parentheses?\" These are questions I had as I wrote. Learning the answers was fascinating and helped my text become much clearer. Be Brief: Rule of 3s - If you don't have a rule of thumb when writing scientific reports, use the rule of 3s. Put simply, this states there must be \u2265 3 words to a sentence, \u2265 3 sentences to a paragraph, and \u2265 3 paragraphs to a page. This rule is very harsh and is intended to enforce structure to your sentences and paragraphs, especially when used in tandem with the next point: topic and linkage sentences. Topic & Linkage Sentences - Your first sentence should tell the reader the primary message of the paragraph. Your last sentence should directly lead onto the topic sentence of the next paragraph. You middle sentence(s) are the 'body' of the argument raised in the first sentence. This structure enforces flow within and between paragraphs. Topic and linkage sentences are hard to achieve when you start (often you fix these when editing), but eventually, the use of this structure makes you a better writer and helps you present clear, strong arguments --- because every paragraph will clearly relate to one-another. Active voice - Use active voice: [e.g., Littel et al (2018) found that X impacts Y.], not passive voice [e.g., X has been shown to impact Y (Little et al, 2018)]. Active voice is engaging, it makes you want to read on! Passive voice is dispassionate and rather dull -- not something you want for your 5000 word essay! Use sub-headings - Sub-headings are a roadmap to your work, especially in scientific writing. If you are writing anything over 1500 words; you should probably have sub-headings. Not only do sub-headings give your reader a chance to pause and collect their thoughts, they provide a clear description of how your argument was made and where your marks should be allocated. Anything to help your marker give 'more marks' or better understand your argument is a good thing. These tips are small and simple, however, I have no doubt that they helped me write my 50,000 word PhD thesis and three academic articles in the space of six months. I endorse each of these tips fully, and hope they help other writers put words to page and maybe even learn to enjoy the writing process.","title":"Writing Better"},{"location":"Projects/","text":"Projects My academic work covers a range of topics and has been summarised below. My research includes mathematical advancements and simulation work in Systems Factorial Technology, using experience sampling methods to investigate human memory, using non-parametric frameworks and multidimensional scaling to understand numerical cognition in English and Chinese speakers, and examining visual word processing efficiency for Chinese and English words. Some of these projects are on-going (Current Projects), while others are completed (Past Projects); all are presented below. Current Projects Word processing English and Chinese word processing efficiency When we read, we process a limited number of letters and characters to form a single representation \u2013 a word. Whether we process words through a part-based or holistic visual process is a matter of conjecture, and may vary across writing systems. English words are alphabetic and may be processed in a more holistic manner than non-alphabetic words, such as Chinese characters. In the current project, Yang, Lin and I used response-times to examine visual word processing efficiency \u2013 a measure of holistic processing \u2013 in a Chinese speaking cohort, while matching Chinese characters and English words. Word type (word, pseudo-word, and non-word) and task instructions (recognize an identical vs different word) were manipulated, and processing efficiency was measured separately for whole word (AND) and word-part (OR) processing strategies. This work is on-going. Watch this space to learn how the results turn out! Numerical cognition Expertise and the mental space Symbolic numerals have been developed by various cultures to quickly and accurately convey quantity. People sometimes confuse one numeral for another, for example, 6 with 9. These confusion patterns relate to the distances between our mental representations --- hypothetical internal spaces thought to reflect the external world. Numerals separated by smaller distances in the mental space are easier to confuse. These distances may represent perceptual similarity or numerical proximity and may change with experience. For example, numerical proximity is only relevant after learning the numerical order. In a Chinese speaking cohort, Bennett, Howard, Hsieh, Yang, Little, Eidels and I investigated the mental representations of familiar and unfamiliar numerals (4 sets: Arabic, Chinese, Thai, and non-symbolic dots; see Figure 8) through a set of identification experiments, using multi-dimensional scaling and cluster analysis. We controlled for undesired effects of response bias using Luce\u2019s choice model. To assess the effect of expertise, we compare the results of this study to those of an English speaking cohort (described in the following project). Across cohorts, our findings show Arabic, Chinese and Thai numerals were confused due to perceptual similarities. Non-symbolic dots were confused due to numerical and perceptual similarities. Mental representations differed between cohorts for Chinese numerals due to the different levels of expertise (see Figure 9). Figure 9. Group indscal MDS solutions for the Chinese speaking cohort (left), English speaking cohort (middle) and a procrustes analysis fitting the English cohort to the Chinese cohort's MDS solutions (right). Colored items represent the English cohort's transformed MDS indscal solution fit to the Chinese MDS indscal solution. Standardised sum of squared errors (goodness of fit) is reported in the top left of each panel; a lower SSerror indicates a better fit between cohorts. The mental representation of Chinese digits differed most due to differences in the levels of Cohort expertise. Subjective interpretations describing how or why items were confused in each plot are presented on the external X- and Y-axes. Dot digits are displayed as Arabic numerals to avoid confusing these digits within the plot. The mental representation of numerals People express quantities using a remarkably small set of units \u2013 digits. Confusing digits could be costly, and not all confusions are equal; confusing a price tag of 2 dollars with 9 dollars is naturally more costly than confusing 2 with 3. Confusion patterns are intimately related to the distances between mental representations, which are hypothetical internal symbols said to stand for, or represent, 'real' external stimuli. The distance between the mental representations of two digits could be determined by their numerical distance. Alternatively, it could be driven by visual similarity. In an English speaking cohort, Bennett, Howard, Yang, Little, Eidels and I investigated the mental representations of familiar and unfamiliar numbers (4 sets: Arabic, Chinese, Thai, and non-symbolic dots; Figure 8 top) through a set of identification experiments (example, Figure 8 bottom), using multi-dimensional scaling and cluster analysis. We controlled for undesired effects of response bias using Luce\u2019s choice model. Our findings show Arabic, Chinese and Thai numerals were represented in the mental space by perceptual similarities. We also find non-symbolic dots were represented by perceptual and numerical similarities. Read more about this in our preprint The cost of errors: confusion analysis and the mental representation of familiar and unfamiliar digits , in review at Acta Psychologica. Figure 8. (Top) stimuli for the four language types used in the experiment. (Bottom) Example of a trial; stimulus noise was degraded through a staircase procedure, and participants reported which item was presented by moving a mouse to the correct sector of the response-wheel. Correct (green) and error (red) responses were collected for each stimuli (response-proportions displayed by outer blue values) to create a 9 x 9 digit confusion matrix for each individual in each language condition. Past Projects Advancements to Systems Factorial Technology Mixture Models Human information processing is flexible in its ability to utilize mechanisms such as attention and memory along with basic perceptual processes. As a consequence, information processing is probably best thought of as not reflecting one type of standard system or architecture, but as a mixture of different types of systems. We examine the predictions of mixtures of different processing models using Systems Factorial Technology (Townsend and Nozawa, 1995). SFT offers a number of important diagnostic measures for differentiating pure processing models (e.g., serial or parallel). Little, Eidles, Houpt, Griffiths and I show that mixtures of basic processes result in smooth, gradual changes to these measures reflecting the proportions of each process (Figure 7). The identifiability of these mixtures, in comparison to interactive parallel channel models, is discussed with reference to the fixed-point property of mixture models. Read more in our article Systems Factorial Technology analysis of mixtures of processing architectures published in the Journal of Mathematical Psychology. Figure 7. Top: survivor interaction contrast (SIC) predictions for the self-terminating coactive/parallel (left) and self-terminating serial/parallel (right) mixture models. Bottom: survivor interaction contrast (SIC) predictions for the exhaustive coactive/parallel (left) and exhaustive serial/parallel (right) mixture models. Simulations of each component process were instantiated as a pair of Poisson accumulators. Nice Guys Check Twice Systems Factorial Technology (SFT) is a popular framework for that has been used to investigate processing capacity across many psychological domains over the past 25+ years. To date, it had been assumed that no processing resources are used for sources in which no signal has been presented (i.e., in a location that can contain a signal but does not on a given trial). Hence, response times are purely driven by the \u201csignal-containing\u201d location or locations. This assumption is critical to the underlying mathematics of the capacity coefficient measure of SFT. In this project, Howard, Little, Townsend, Eidles and I show that stimulus locations influence response times even when they contain no signal, and that this influence has repercussions for the interpretation of processing capacity under the SFT framework, particularly in conjunctive (AND) tasks - where positive responses require detection of signals in multiple locations. We propose a modification to the AND task requiring participants to fully identify both target locations on all trials. This modification allows a new coefficient to be derived. We apply the new coefficient to novel experimental data and resolve a previously reported empirical paradox, where observed capacity was limited in an OR detection task but super capacity in an AND detection task (see Figure 6). Hence, previously reported differences in processing capacity between OR and AND task designs are likely to have been spurious. Read more in our preprint Nice Guys Check Twice: No-Signal Processes in Systems Factorial Technology , submitted to Psych Review. Figure 6. Group level averaged capacity coefficients for the same participants completing tasks requiring the OR-, AND-, and our new ID capacity coefficient. The coloured regions represent 1000 bootstrapped average capacity coefficients for each of the tasks. OR and ID capacity coefficients are similar to one-another whereas the AND coefficient is disparate. Numerical cognition Systems of Estimation Like many species, humans can perform non-verbal estimates of quantity through our innate approximate number system. However, the cognitive mechanisms that govern how we compare these estimates are not well understood. Little research has addressed how the human estimation-system evaluates multiple quantities, and fewer studies have considered the cost to cognitive workload when undertaking such a task. Here, we provide a novel application of Systems Factorial Technology (SFT; Townsend and Nozawa, 1995) to a comparative estimation task. Across a series of four experiments, Howard, Houpt, Landy, Eidels and I assess whether quantities, (i.e. non-overlapping red and blue discs), are estimated simultaneously (in parallel) or sequentially (in serial), and under what restrictions to cognitive workload. Our findings reveal that two item-sets may be estimated simultaneously through a parallel estimation system, under severe restrictions to cognitive workload capacity. These restrictions were so severe, as to make the parallel estimation of two quantities less efficient than the estimation of each quantity in succession. While the estimation of a single item-set may be colloquially considered an effortless process, our results show that the estimation of multiple item-sets is a rather demanding feat. Read more in our manuscript Comparative estimation systems perform under severely limited workload capacity published in the Journal of Mathematical Psychology. Systems of Subitizing Whether we realise it or not, many everyday comparisons of quantity occur within the subitizing range, 1--4. For example, when we choose the fewer of two coffee queues, we may rapidly subitize each queue before deciding which one to join. As we go to pay, we might empty our wallet of coins and subitize the gold coins from amongst the silver shrapnel. In both of these instances, we are using a subitizing system to evaluate item-sets when those sets are physically separate or intermixed. But under what processing architecture and workload capacity does this subitizing system operate? In this project, Thorpe, Landy, Houpt, Eidels and I investigate whether two small item-sets (see Figure 5) may be subitized at the same time through a parallel subitizing system. We find that, contrary to our expectations, two item-sets must be subitized one-after-another in a serial processing system. We observe that processing operates under severe limitations to workload capacity (processing efficiency). This finding indicates the presence of additional context effects that slow the subitizing of two item-sets beyond the sum processing-times expected for subitizing each item-set in isolation. These findings held across conditions of separation and across manipulations of item-set area (see Figure 5). Figure 5. Example stimuli for when participants had to identify if either color set contained less-three three discs, when discs were of a fixed size (left), and when item-set area was fixed (right); for mixed and separate item-set designs. Each example illustrates a conflict-target response-condition, where one color-set was fewer than three, and one color-set was greater than three. Memory Hierarchical Bayesian model of memory for 'when' In this project, Dennis, Yim, Sreekumar, Evans, Stone, Sederberg and I asked participants to wear a smartphone, which collected GPS, audio, accelerometry and image data, in a pouch around their necks for a period of two weeks (Figure 4; left). After a retention interval of one week, they were asked to judge the specific day on which each of a selection of images was taken. To account for people\u2019s judgements, we proposed a mixture model of four processes - uniform guessing, a signal detection process based on decaying memory strength, a week confusion process and a event confusion process in which the sensor streams were used to calculate the similarity of events. A model selection exercise testing all possible subsets of the processes favoured a model that included only the event confusion model (Figure 4; middle). GPS similarities were found to be the most significant predictors of memory, followed by audio and accelerometry similarities and then image similarities (Figure 4; right). To read more on this, see our article A hierarchical Bayesian model of \u201cmemory for when\u201d based on experience sampling data . Figure 4. Participant wearing smart-phone (left), sensory event based model that confused memories based upon day proximity (middle) and associated predictive weights of each sensor (right). An Unforgettable Experience Online and sensor technologies promise to transform many areas of psychological enquiry. However, collecting and analyzing such data are challenging. In this project, Dennis, Yim, Sreekumar, Stone and I introduce the unforgettable.me experience-sampling platform. Unforgettable.me includes an app that can collect image, Global Positioning System, accelerometry, and audio data in a continuous fashion and upload the data to a server. The data are then automatically augmented by using online databases to identify the address, type of location, and weather conditions, as well as provide street view imagery. In addition, machine-learning classifiers are run to identify aspects of the audio data such as voice and traffic. The augmented data are available to participants in the form of a keyword search interface, as well as via several visualization mechanisms. In addition, Unforgettable Research Services partners with If This Then That (IFTTT), and so can accumulate data from any of over 600 sources, including social media, wearables, and other devices. Through IFTTT, buttons can be added as icons to smartphones to allow participants to register mood conveniently, as well as behaviors and physiological states such as happiness, microaggressions, or illness. Furthermore, unforgettable.me incorporates a mechanism that allows researchers to run experiments and analyze data within an authenticated environment without viewing users' private data. To read more on this, see our article A system for collecting and analyzing experience sampling data. published in Behaviour Research Methods or visit the Unforgettable.me website. Word processing Stroop effect: Fixed point analysis For the last 80 years, the Stroop task has been used to test theories of attention and cognitive control. The Stroop task involves naming the print color of a word, where the word itself is typically the name of a color (e.g., the word GREEN printed in red print requires a response of 'red\u2019; Stroop, 1935). People are faster at naming the print color when it matches the word (congruent stimuli, e.g., RED in red) compared to when the word and print color do not match (incongruent stimuli, GREEN in red). Most theories posit that the overwhelming power of written word overcomes the strict instructions to focus on print color. Recent evidence suggests that trials in the Stroop task could in fact be a mixture of reading trials and non-reading trials. Identifying mixtures of two processes is a notoriously difficult task, with the diagnosis of mixture processes requiring strict assumptions to hold. In this project, Tillman, Howard, Eidels and I conduct a critical test of the Stroop reading-color mixture process. We assume that, given these mixtures are linearly additive, a mixture of these processes should satisfy the fixed-point property (Falmagne, 1968). Although we found some evidence for a mixture of distributions using the fixed-point analysis in the incongruent condition, the results of this project were not conclusive. To read more about this project, see our article The Stroop Effect From a Mixture of Reading Processes: A Fixed-Point Analysis , published in the Proceedings of the 39th Annual Conference of the Cognitive Science Society. Figure 3. Predictions of the fixed point property (top) and overall RT density for congruent and incongruent Stroop distributions (bottom). Some evidence for a mixture of distributions using the fixed-point analysis was found for the incongruent condition, but not for the congruent condition. Open Science Privacy vs Open Science Pervasive internet and sensor technologies promise to revolutionize psychological science. For example, my recent work with Dennis, Yim, Stone and collogues allows the passive collection of GPS, accelerometry, image, and audio data from participant's smart-phones. The data collected using internet and sensor technologies are often very personal and the value of this data increases with its sensitivity (e.g., email meta data vs medical record data). At the same time, driven by the replication crisis, there is a sustained push to publish data to open repositories. These movements are in fundamental conflict. In this project, Dennis Yim, Hamm, Osth, Sreekumar, Stone and I propose a way to navigate this issue. We argue that there are significant advantages to be gained by ceding the ownership of data to the participants who generate the data. We then provide desiderata for a privacy-preserving platform called Unforgettable.me. In particular, we suggest that researchers should use an interface to perform experiments and run analyses, rather than observing the stimuli themselves. We argue that this method not only improves privacy but will also encourage greater compliance with good research practices than is possible through open repositories. To read more on this, see our article Privacy vs Open Science , published in Behaviour Research Methods. Task Switching Measuring uncertainty in switch costs Cognitive control is required in situations that involve uncertainty or change, such as when resolving conflict, selecting responses and switching tasks. It has been suggested that cognitive control can be conceptualised as a mechanism which prioritises goal-relevant information to deal with uncertainty. This hypothesis has been supported using a paradigm that requires conflict resolution. In this project, Cooper, Rennie, Karayanidis and I examined whether cognitive control during task switching is also consistent with this notion. Using information theory, we quantified the level of uncertainty in different trial types during a cued task-switching paradigm (see Figure 1). We test the hypothesis that differences in uncertainty between task repeat and task switch trials can account for typical behavioural effects in task-switching. Increasing uncertainty was associated with less efficient performance (i.e., slower and less accurate; see Figure 2), particularly on switch trials and trials that afford little opportunity for advance preparation. Interestingly, both mixing and switch costs were associated with a common episodic control process. Our entropy algorithm was then generalized and fit to 20 other experiments from 12 different studies, proving to be a good predictor of behavioural performance across all experiments. The results of this project support the notion that cognitive control may be conceptualised as an information processor that serves to resolve uncertainty in the environment. To read more on this, see our published manuscript Task Uncertainty Can Account for Mixing and Switch Costs in Task-Switching published in Plos One. Figure 1. A) Timeline of a specific trial. Response-cue interval (RCI) and cue-target interval are fixed at 400 ms and 1000 ms, respectively. On each trial, the cue highlights two of the six segments of the circle and indicates that the target will appear in one of these two segments. In this instance, the cue covers both \u2018letter task\u2019 segments, and the participant can prepare to apply the \u2018letter\u2019 task rules on the upcoming target. When the target (e.g., A4) appears in a letter task segment, participants must respond to the task-relevant feature of the target (e.g., the letter A is a vowel, respond with left hand), and ignore the task-irrelevant feature of the target (e.g., the number 4). B) The progression from trial N-1 to trial N defines the trial type. i) If, having completed the letter task on trial N-1, the same segment is highlighted on trial N, it is a repeat trial and the participant will repeat the letter task. ii) If the cue highlights both segments of one of the other tasks, it is a switch-to trial. The target will appear in one of the two highlighted segments, and the participant can use the CTI to update the new task rules (e.g., digit task, in this example). iii) If the cue highlights adjoining segments of the two tasks not completed on trial N-1 (e.g., digit and color), it is a switch-away trial. The target is equally likely to appear in the digit and color segments and the participant can prepare to switch task (e.g., not repeat the letter task), but does not know which task to upload until the target appears. The position of the target indicates which task to complete. iv) If the cue highlights one segment from the task completed on trial N-1 (e.g., letter task) and one from another task (e.g., digit task), it is a non-informative trial. The target is equally likely to appear in the letter or the digit segment, and require either a repeat (non-informative repeat) or a switch (non-informative switch) in task. C) Each major segment of the wheel is consistently mapped to one of the three tasks: letter, digit and color. The table shows the eight exemplars used for each task and an example of stimulus-response mappings. D) Exemplar differences between a repeat trial for Experiment 1 and Experiment 2, whereby an additional bivalent distractor is presented at target onset in a non-cued section of the wheel during Experiment 2. Figure 2. Information processing requirements (in bits) were assumed to differ based on an increased entropy for mixed blocks in contrast to single blocks, with the addition of an extra degree of uncertainty for switching within a mixed-task block. Our algorithm strongly predicted mixing and switch costs for accuracy in both Experiment 1 (A: r2 = .886, p = .005) and Experiment 2 (B: r2 = .701, p = .038) and for RT in both Experiment 1 (A: r2 = .934, p = .002) and Experiment 2 (B: r2 = .924, p = .002). SBR; single block repeat, MR; mixed repeat, ST; switch-to, SA; switch-away, NR; non-informative repeat, NS; non-informative switch. \u2013D denotes distractor paradigm trials.","title":"Academic Projects"},{"location":"Projects/#projects","text":"My academic work covers a range of topics and has been summarised below. My research includes mathematical advancements and simulation work in Systems Factorial Technology, using experience sampling methods to investigate human memory, using non-parametric frameworks and multidimensional scaling to understand numerical cognition in English and Chinese speakers, and examining visual word processing efficiency for Chinese and English words. Some of these projects are on-going (Current Projects), while others are completed (Past Projects); all are presented below.","title":"Projects"},{"location":"Projects/#current-projects","text":"","title":"Current Projects"},{"location":"Projects/#word-processing","text":"","title":"Word processing"},{"location":"Projects/#english-and-chinese-word-processing-efficiency","text":"When we read, we process a limited number of letters and characters to form a single representation \u2013 a word. Whether we process words through a part-based or holistic visual process is a matter of conjecture, and may vary across writing systems. English words are alphabetic and may be processed in a more holistic manner than non-alphabetic words, such as Chinese characters. In the current project, Yang, Lin and I used response-times to examine visual word processing efficiency \u2013 a measure of holistic processing \u2013 in a Chinese speaking cohort, while matching Chinese characters and English words. Word type (word, pseudo-word, and non-word) and task instructions (recognize an identical vs different word) were manipulated, and processing efficiency was measured separately for whole word (AND) and word-part (OR) processing strategies. This work is on-going. Watch this space to learn how the results turn out!","title":"English and Chinese word processing efficiency"},{"location":"Projects/#numerical-cognition","text":"","title":"Numerical cognition"},{"location":"Projects/#expertise-and-the-mental-space","text":"Symbolic numerals have been developed by various cultures to quickly and accurately convey quantity. People sometimes confuse one numeral for another, for example, 6 with 9. These confusion patterns relate to the distances between our mental representations --- hypothetical internal spaces thought to reflect the external world. Numerals separated by smaller distances in the mental space are easier to confuse. These distances may represent perceptual similarity or numerical proximity and may change with experience. For example, numerical proximity is only relevant after learning the numerical order. In a Chinese speaking cohort, Bennett, Howard, Hsieh, Yang, Little, Eidels and I investigated the mental representations of familiar and unfamiliar numerals (4 sets: Arabic, Chinese, Thai, and non-symbolic dots; see Figure 8) through a set of identification experiments, using multi-dimensional scaling and cluster analysis. We controlled for undesired effects of response bias using Luce\u2019s choice model. To assess the effect of expertise, we compare the results of this study to those of an English speaking cohort (described in the following project). Across cohorts, our findings show Arabic, Chinese and Thai numerals were confused due to perceptual similarities. Non-symbolic dots were confused due to numerical and perceptual similarities. Mental representations differed between cohorts for Chinese numerals due to the different levels of expertise (see Figure 9). Figure 9. Group indscal MDS solutions for the Chinese speaking cohort (left), English speaking cohort (middle) and a procrustes analysis fitting the English cohort to the Chinese cohort's MDS solutions (right). Colored items represent the English cohort's transformed MDS indscal solution fit to the Chinese MDS indscal solution. Standardised sum of squared errors (goodness of fit) is reported in the top left of each panel; a lower SSerror indicates a better fit between cohorts. The mental representation of Chinese digits differed most due to differences in the levels of Cohort expertise. Subjective interpretations describing how or why items were confused in each plot are presented on the external X- and Y-axes. Dot digits are displayed as Arabic numerals to avoid confusing these digits within the plot.","title":"Expertise and the mental space"},{"location":"Projects/#the-mental-representation-of-numerals","text":"People express quantities using a remarkably small set of units \u2013 digits. Confusing digits could be costly, and not all confusions are equal; confusing a price tag of 2 dollars with 9 dollars is naturally more costly than confusing 2 with 3. Confusion patterns are intimately related to the distances between mental representations, which are hypothetical internal symbols said to stand for, or represent, 'real' external stimuli. The distance between the mental representations of two digits could be determined by their numerical distance. Alternatively, it could be driven by visual similarity. In an English speaking cohort, Bennett, Howard, Yang, Little, Eidels and I investigated the mental representations of familiar and unfamiliar numbers (4 sets: Arabic, Chinese, Thai, and non-symbolic dots; Figure 8 top) through a set of identification experiments (example, Figure 8 bottom), using multi-dimensional scaling and cluster analysis. We controlled for undesired effects of response bias using Luce\u2019s choice model. Our findings show Arabic, Chinese and Thai numerals were represented in the mental space by perceptual similarities. We also find non-symbolic dots were represented by perceptual and numerical similarities. Read more about this in our preprint The cost of errors: confusion analysis and the mental representation of familiar and unfamiliar digits , in review at Acta Psychologica. Figure 8. (Top) stimuli for the four language types used in the experiment. (Bottom) Example of a trial; stimulus noise was degraded through a staircase procedure, and participants reported which item was presented by moving a mouse to the correct sector of the response-wheel. Correct (green) and error (red) responses were collected for each stimuli (response-proportions displayed by outer blue values) to create a 9 x 9 digit confusion matrix for each individual in each language condition.","title":"The mental representation of numerals"},{"location":"Projects/#past-projects","text":"","title":"Past Projects"},{"location":"Projects/#advancements-to-systems-factorial-technology","text":"","title":"Advancements to Systems Factorial Technology"},{"location":"Projects/#mixture-models","text":"Human information processing is flexible in its ability to utilize mechanisms such as attention and memory along with basic perceptual processes. As a consequence, information processing is probably best thought of as not reflecting one type of standard system or architecture, but as a mixture of different types of systems. We examine the predictions of mixtures of different processing models using Systems Factorial Technology (Townsend and Nozawa, 1995). SFT offers a number of important diagnostic measures for differentiating pure processing models (e.g., serial or parallel). Little, Eidles, Houpt, Griffiths and I show that mixtures of basic processes result in smooth, gradual changes to these measures reflecting the proportions of each process (Figure 7). The identifiability of these mixtures, in comparison to interactive parallel channel models, is discussed with reference to the fixed-point property of mixture models. Read more in our article Systems Factorial Technology analysis of mixtures of processing architectures published in the Journal of Mathematical Psychology. Figure 7. Top: survivor interaction contrast (SIC) predictions for the self-terminating coactive/parallel (left) and self-terminating serial/parallel (right) mixture models. Bottom: survivor interaction contrast (SIC) predictions for the exhaustive coactive/parallel (left) and exhaustive serial/parallel (right) mixture models. Simulations of each component process were instantiated as a pair of Poisson accumulators.","title":"Mixture Models"},{"location":"Projects/#nice-guys-check-twice","text":"Systems Factorial Technology (SFT) is a popular framework for that has been used to investigate processing capacity across many psychological domains over the past 25+ years. To date, it had been assumed that no processing resources are used for sources in which no signal has been presented (i.e., in a location that can contain a signal but does not on a given trial). Hence, response times are purely driven by the \u201csignal-containing\u201d location or locations. This assumption is critical to the underlying mathematics of the capacity coefficient measure of SFT. In this project, Howard, Little, Townsend, Eidles and I show that stimulus locations influence response times even when they contain no signal, and that this influence has repercussions for the interpretation of processing capacity under the SFT framework, particularly in conjunctive (AND) tasks - where positive responses require detection of signals in multiple locations. We propose a modification to the AND task requiring participants to fully identify both target locations on all trials. This modification allows a new coefficient to be derived. We apply the new coefficient to novel experimental data and resolve a previously reported empirical paradox, where observed capacity was limited in an OR detection task but super capacity in an AND detection task (see Figure 6). Hence, previously reported differences in processing capacity between OR and AND task designs are likely to have been spurious. Read more in our preprint Nice Guys Check Twice: No-Signal Processes in Systems Factorial Technology , submitted to Psych Review. Figure 6. Group level averaged capacity coefficients for the same participants completing tasks requiring the OR-, AND-, and our new ID capacity coefficient. The coloured regions represent 1000 bootstrapped average capacity coefficients for each of the tasks. OR and ID capacity coefficients are similar to one-another whereas the AND coefficient is disparate.","title":"Nice Guys Check Twice"},{"location":"Projects/#numerical-cognition_1","text":"","title":"Numerical cognition"},{"location":"Projects/#systems-of-estimation","text":"Like many species, humans can perform non-verbal estimates of quantity through our innate approximate number system. However, the cognitive mechanisms that govern how we compare these estimates are not well understood. Little research has addressed how the human estimation-system evaluates multiple quantities, and fewer studies have considered the cost to cognitive workload when undertaking such a task. Here, we provide a novel application of Systems Factorial Technology (SFT; Townsend and Nozawa, 1995) to a comparative estimation task. Across a series of four experiments, Howard, Houpt, Landy, Eidels and I assess whether quantities, (i.e. non-overlapping red and blue discs), are estimated simultaneously (in parallel) or sequentially (in serial), and under what restrictions to cognitive workload. Our findings reveal that two item-sets may be estimated simultaneously through a parallel estimation system, under severe restrictions to cognitive workload capacity. These restrictions were so severe, as to make the parallel estimation of two quantities less efficient than the estimation of each quantity in succession. While the estimation of a single item-set may be colloquially considered an effortless process, our results show that the estimation of multiple item-sets is a rather demanding feat. Read more in our manuscript Comparative estimation systems perform under severely limited workload capacity published in the Journal of Mathematical Psychology.","title":"Systems of Estimation"},{"location":"Projects/#systems-of-subitizing","text":"Whether we realise it or not, many everyday comparisons of quantity occur within the subitizing range, 1--4. For example, when we choose the fewer of two coffee queues, we may rapidly subitize each queue before deciding which one to join. As we go to pay, we might empty our wallet of coins and subitize the gold coins from amongst the silver shrapnel. In both of these instances, we are using a subitizing system to evaluate item-sets when those sets are physically separate or intermixed. But under what processing architecture and workload capacity does this subitizing system operate? In this project, Thorpe, Landy, Houpt, Eidels and I investigate whether two small item-sets (see Figure 5) may be subitized at the same time through a parallel subitizing system. We find that, contrary to our expectations, two item-sets must be subitized one-after-another in a serial processing system. We observe that processing operates under severe limitations to workload capacity (processing efficiency). This finding indicates the presence of additional context effects that slow the subitizing of two item-sets beyond the sum processing-times expected for subitizing each item-set in isolation. These findings held across conditions of separation and across manipulations of item-set area (see Figure 5). Figure 5. Example stimuli for when participants had to identify if either color set contained less-three three discs, when discs were of a fixed size (left), and when item-set area was fixed (right); for mixed and separate item-set designs. Each example illustrates a conflict-target response-condition, where one color-set was fewer than three, and one color-set was greater than three.","title":"Systems of Subitizing"},{"location":"Projects/#memory","text":"","title":"Memory"},{"location":"Projects/#hierarchical-bayesian-model-of-memory-for-when","text":"In this project, Dennis, Yim, Sreekumar, Evans, Stone, Sederberg and I asked participants to wear a smartphone, which collected GPS, audio, accelerometry and image data, in a pouch around their necks for a period of two weeks (Figure 4; left). After a retention interval of one week, they were asked to judge the specific day on which each of a selection of images was taken. To account for people\u2019s judgements, we proposed a mixture model of four processes - uniform guessing, a signal detection process based on decaying memory strength, a week confusion process and a event confusion process in which the sensor streams were used to calculate the similarity of events. A model selection exercise testing all possible subsets of the processes favoured a model that included only the event confusion model (Figure 4; middle). GPS similarities were found to be the most significant predictors of memory, followed by audio and accelerometry similarities and then image similarities (Figure 4; right). To read more on this, see our article A hierarchical Bayesian model of \u201cmemory for when\u201d based on experience sampling data . Figure 4. Participant wearing smart-phone (left), sensory event based model that confused memories based upon day proximity (middle) and associated predictive weights of each sensor (right).","title":"Hierarchical Bayesian model of memory for 'when'"},{"location":"Projects/#an-unforgettable-experience","text":"Online and sensor technologies promise to transform many areas of psychological enquiry. However, collecting and analyzing such data are challenging. In this project, Dennis, Yim, Sreekumar, Stone and I introduce the unforgettable.me experience-sampling platform. Unforgettable.me includes an app that can collect image, Global Positioning System, accelerometry, and audio data in a continuous fashion and upload the data to a server. The data are then automatically augmented by using online databases to identify the address, type of location, and weather conditions, as well as provide street view imagery. In addition, machine-learning classifiers are run to identify aspects of the audio data such as voice and traffic. The augmented data are available to participants in the form of a keyword search interface, as well as via several visualization mechanisms. In addition, Unforgettable Research Services partners with If This Then That (IFTTT), and so can accumulate data from any of over 600 sources, including social media, wearables, and other devices. Through IFTTT, buttons can be added as icons to smartphones to allow participants to register mood conveniently, as well as behaviors and physiological states such as happiness, microaggressions, or illness. Furthermore, unforgettable.me incorporates a mechanism that allows researchers to run experiments and analyze data within an authenticated environment without viewing users' private data. To read more on this, see our article A system for collecting and analyzing experience sampling data. published in Behaviour Research Methods or visit the Unforgettable.me website.","title":"An Unforgettable Experience"},{"location":"Projects/#word-processing_1","text":"","title":"Word processing"},{"location":"Projects/#stroop-effect-fixed-point-analysis","text":"For the last 80 years, the Stroop task has been used to test theories of attention and cognitive control. The Stroop task involves naming the print color of a word, where the word itself is typically the name of a color (e.g., the word GREEN printed in red print requires a response of 'red\u2019; Stroop, 1935). People are faster at naming the print color when it matches the word (congruent stimuli, e.g., RED in red) compared to when the word and print color do not match (incongruent stimuli, GREEN in red). Most theories posit that the overwhelming power of written word overcomes the strict instructions to focus on print color. Recent evidence suggests that trials in the Stroop task could in fact be a mixture of reading trials and non-reading trials. Identifying mixtures of two processes is a notoriously difficult task, with the diagnosis of mixture processes requiring strict assumptions to hold. In this project, Tillman, Howard, Eidels and I conduct a critical test of the Stroop reading-color mixture process. We assume that, given these mixtures are linearly additive, a mixture of these processes should satisfy the fixed-point property (Falmagne, 1968). Although we found some evidence for a mixture of distributions using the fixed-point analysis in the incongruent condition, the results of this project were not conclusive. To read more about this project, see our article The Stroop Effect From a Mixture of Reading Processes: A Fixed-Point Analysis , published in the Proceedings of the 39th Annual Conference of the Cognitive Science Society. Figure 3. Predictions of the fixed point property (top) and overall RT density for congruent and incongruent Stroop distributions (bottom). Some evidence for a mixture of distributions using the fixed-point analysis was found for the incongruent condition, but not for the congruent condition.","title":"Stroop effect: Fixed point analysis"},{"location":"Projects/#open-science","text":"","title":"Open Science"},{"location":"Projects/#privacy-vs-open-science","text":"Pervasive internet and sensor technologies promise to revolutionize psychological science. For example, my recent work with Dennis, Yim, Stone and collogues allows the passive collection of GPS, accelerometry, image, and audio data from participant's smart-phones. The data collected using internet and sensor technologies are often very personal and the value of this data increases with its sensitivity (e.g., email meta data vs medical record data). At the same time, driven by the replication crisis, there is a sustained push to publish data to open repositories. These movements are in fundamental conflict. In this project, Dennis Yim, Hamm, Osth, Sreekumar, Stone and I propose a way to navigate this issue. We argue that there are significant advantages to be gained by ceding the ownership of data to the participants who generate the data. We then provide desiderata for a privacy-preserving platform called Unforgettable.me. In particular, we suggest that researchers should use an interface to perform experiments and run analyses, rather than observing the stimuli themselves. We argue that this method not only improves privacy but will also encourage greater compliance with good research practices than is possible through open repositories. To read more on this, see our article Privacy vs Open Science , published in Behaviour Research Methods.","title":"Privacy vs Open Science"},{"location":"Projects/#task-switching","text":"","title":"Task Switching"},{"location":"Projects/#measuring-uncertainty-in-switch-costs","text":"Cognitive control is required in situations that involve uncertainty or change, such as when resolving conflict, selecting responses and switching tasks. It has been suggested that cognitive control can be conceptualised as a mechanism which prioritises goal-relevant information to deal with uncertainty. This hypothesis has been supported using a paradigm that requires conflict resolution. In this project, Cooper, Rennie, Karayanidis and I examined whether cognitive control during task switching is also consistent with this notion. Using information theory, we quantified the level of uncertainty in different trial types during a cued task-switching paradigm (see Figure 1). We test the hypothesis that differences in uncertainty between task repeat and task switch trials can account for typical behavioural effects in task-switching. Increasing uncertainty was associated with less efficient performance (i.e., slower and less accurate; see Figure 2), particularly on switch trials and trials that afford little opportunity for advance preparation. Interestingly, both mixing and switch costs were associated with a common episodic control process. Our entropy algorithm was then generalized and fit to 20 other experiments from 12 different studies, proving to be a good predictor of behavioural performance across all experiments. The results of this project support the notion that cognitive control may be conceptualised as an information processor that serves to resolve uncertainty in the environment. To read more on this, see our published manuscript Task Uncertainty Can Account for Mixing and Switch Costs in Task-Switching published in Plos One. Figure 1. A) Timeline of a specific trial. Response-cue interval (RCI) and cue-target interval are fixed at 400 ms and 1000 ms, respectively. On each trial, the cue highlights two of the six segments of the circle and indicates that the target will appear in one of these two segments. In this instance, the cue covers both \u2018letter task\u2019 segments, and the participant can prepare to apply the \u2018letter\u2019 task rules on the upcoming target. When the target (e.g., A4) appears in a letter task segment, participants must respond to the task-relevant feature of the target (e.g., the letter A is a vowel, respond with left hand), and ignore the task-irrelevant feature of the target (e.g., the number 4). B) The progression from trial N-1 to trial N defines the trial type. i) If, having completed the letter task on trial N-1, the same segment is highlighted on trial N, it is a repeat trial and the participant will repeat the letter task. ii) If the cue highlights both segments of one of the other tasks, it is a switch-to trial. The target will appear in one of the two highlighted segments, and the participant can use the CTI to update the new task rules (e.g., digit task, in this example). iii) If the cue highlights adjoining segments of the two tasks not completed on trial N-1 (e.g., digit and color), it is a switch-away trial. The target is equally likely to appear in the digit and color segments and the participant can prepare to switch task (e.g., not repeat the letter task), but does not know which task to upload until the target appears. The position of the target indicates which task to complete. iv) If the cue highlights one segment from the task completed on trial N-1 (e.g., letter task) and one from another task (e.g., digit task), it is a non-informative trial. The target is equally likely to appear in the letter or the digit segment, and require either a repeat (non-informative repeat) or a switch (non-informative switch) in task. C) Each major segment of the wheel is consistently mapped to one of the three tasks: letter, digit and color. The table shows the eight exemplars used for each task and an example of stimulus-response mappings. D) Exemplar differences between a repeat trial for Experiment 1 and Experiment 2, whereby an additional bivalent distractor is presented at target onset in a non-cued section of the wheel during Experiment 2. Figure 2. Information processing requirements (in bits) were assumed to differ based on an increased entropy for mixed blocks in contrast to single blocks, with the addition of an extra degree of uncertainty for switching within a mixed-task block. Our algorithm strongly predicted mixing and switch costs for accuracy in both Experiment 1 (A: r2 = .886, p = .005) and Experiment 2 (B: r2 = .701, p = .038) and for RT in both Experiment 1 (A: r2 = .934, p = .002) and Experiment 2 (B: r2 = .924, p = .002). SBR; single block repeat, MR; mixed repeat, ST; switch-to, SA; switch-away, NR; non-informative repeat, NS; non-informative switch. \u2013D denotes distractor paradigm trials.","title":"Measuring uncertainty in switch costs"},{"location":"Publications/","text":"Publications Peer Reviewed Articles Garrett P. M. , Chiam, P-Y., & Yang, C-T. (In prep). Visual word processing efficiency for Chinese characters and English words. Sreekumar, V., Evans, N. J., Garrett, P. M. , Yim, H., Sederberg, P., & Dennis, S. (In prep). Using an experience sampling approach to distinguish distance versus location-based processing in memory for when. Planned submission: Psychonomic Bulletin & Review . Garrett P. M. , Bennett, M., Howard, Z., Hsieh, Y-.T., Yang, C-.T., Little, D., & Eidels, A. (In prep) Cross cultural symbolic wheel of fortune: an investigation into the mental representation of digits for Chinese and English speakers. Planned submission: Frontiers . Howard, Z., Garrett, P. M. , Little, D., Townsend, J., & Eidels, A. (In review) Nice guys check twice: exhaustive processes in systems factorial technology. Psych Review . Yim, H., Garrett, P. M. , Baker, M., Sreekumar, V., & Dennis, S. (In Review). Examining dependencies among different time scales in episodic memory - an experience sampling study. Journal of memory and language . Garrett P. M. , Bennett, M., Howard, Z., Yang, C-.T., Little, D., & Eidels, A. (In review) The cost of errors: confusion analysis and the mental representation of familiar and unfamiliar digits. Acta Psychologica . Garrett, P. M. , Howard, Z., Landy, D., Houpt, J. W., & Eidels, A. Comparative estimation systems perform under severely limited workload capacity. Journal of mathematical psychology . Dennis, S., Yim, H., Garrett, P. M. , Sreekumar, V., & Stone, B. (2019). A system for collecting and analyzing experience sampling data. Behavioural research methods . Dennis, S., Yim, H., Garrett, P. M. , Sreekumar, V., & Stone, B. (2018). Privacy vs open science. Behavioural research methods . Little, D. R., Eidels, A., Houpt, J. W., Garrett P. M. , & Griffiths, D. W. (2018). Systems Factorial Technology analysis of mixture models. Journal of mathematical psychology (in submission) . Cooper, P.S., Garrett P.M. , Rennie J.L. & Karayanidis F. (2015) Task Uncertainty Can Account for Mixing and Switch Costs in Task-Switching . PLoS ONE . 10(6). Peer Reviewed Conference Proceedings Dennis, S., Garrett, P.M. , Yim, H., Hamm, J., Osth, A., Sreekumar, V., & Stone, B. (2019). Privacy versus Open Science. In Goel, A., Seifert, C., & Freksa, C. (Eds.) Proceedings of the 41st Annual Conference of the Cognitive Science Society . Austin, TX: Cognitive Science Society. Yim, H., Garrett, P. M. , Baker, M., & Dennis, S. (2018). Examining the independence of scales in episodic memory using experience sampling data. Proceedings of the 40th Annual Conference of the Cognitive Science Society (in submission) . Dennis, S., Yim, H., Sreekumar, V., Evans. N., Garrett, P.M. , & Sederberg, P. (2017). A hierarchical Bayesian model of \"memory for when\" based on experience sampling data . Proceedings of the 39th Annual Conference of the Cognitive Science Society . Tillman, G., Howard, Z., Garrett, P.M. , & Eidels, A. (2017). The Stroop Effect From a Mixture of Reading Processes: A Fixed-Point Analysis . Proceedings of the 39th Annual Conference of the Cognitive Science Society . Presentations Conference Presentations Garrett, P. M. , Howard, Z., Houpt, J. W., Landy, D., & Eidles, A. (2019). The comparison of large and small item-sets. The annual meeting of the Australian mathematical society, Melbourne. Garrett, P. M.* , Howard, Z., Houpt, J. W., Landy, D., & Eidles, A. (2018). Estimating multiple item-sets: Harder than you think! Sydney Postgraduate Psychology Conference, University of New South Wales. Winning presentation.*** Garrett, P. M. , Yim, H., Sreekumar, V., Evans, N., & Dennis, S. (2018). A model for `When' in memory. Hunter medical research institute post-graduate conference, Newcastle. Garrett, P. M. , Yim, H., Sreekumar, V., Evans, N., & Dennis, S. (2018). A predictive model for `When' in human memory. Experimental Psychology Conference, Hobart. Garrett, P. M. , Houpt, J., Landy, D., & Eidels, A. (2017). Curve your enthusiasm: advancements in systems of estimation. School of Psychology, The University of Melbourne, Australia. Garrett, P. M. , Waters, L.A., & Eidels, A. (2017). How many jelly beans in two jars? Hunter medical research institute post-graduate conference, Newcastle. Garrett, P. M. , & Eidels, A. (2017). Assessing systems of estimation. The annual meeting of the Australian mathematical society, Brisbane. Garrett, P.M. , Houpt, J., Landy, D., & Eidels, A. (2016). Parallel vs Serial systems of subitizing. Meeting of the 56th Psychonomic Society, Boston. Garrett, P. M. , Houpt, J., Landy, D., & Eidels, A. (2016). Assessing systems of enumeration with SFT (Seminar). School of Psychology, Indiana University, Bloomington. Garrett, P. M. , Thorpe, A., & Eidels, A. (2016). How do you count? Hunter medical research institute post-graduate conference, Newcastle. Garrett, P. M. , & Eidels, A. (2016). A new method for assessing systems of enumeration. The annual meeting of the Australian mathematical society, Hobart. Workshops Garrett, P.M. (2019). The PhD and beyond! Pathways into and out of academia. Graduate researchers in psychological sciences workshop. University of Melbourne. Garrett, P.M. (2018). Claims of causality in health news. Decision making laboratory, University of Melbourne. Garrett, P.M. (2018). Mathematics in Cognitive Psychology. Computer-Assisted Research Mathematics and its Applications (CARMA): mathematical thinking workshop, Newcastle. Conference Posters Garrett, P. M. , Waters, L., Landy, D., Houpt, J., & Eidels, A. (2018) How many jelly beans in two jars? Evidence for the parallel estimation of two quantities. Hunter medical research institute conference for graduate researchers. Garrett, P. M. , Thorpe, A., Landy, D., Houpt, J., & Eidels, A. (2017) A new approach to understanding systems of subitizing - the comparison of small quantities. Experimental psychology conference, Newcastle. Garrett, P. M. , Thorpe, A., Landy, D., Houpt, J., & Eidels, A. (2016). How do you count? Cognitive processing systems of enumeration. 57th annual meeting of the Psychonomics Society, Boston. Garrett, P. M. , Cooper, P., & Karayanidis, F. (2014). Using cognitive control to resolve information context predicts both global and local switch costs. Hunter medical research institute graduate researchers in Psychology conference. Garrett, P. M. , Cooper, P., & Karayanidis, F. (2014). Mid-frontal theta: a measure of attentional processes indexed during cognitive control. Hunter medical research institute satellite conference for the Australian Cognitive Neuroscience Society.","title":"Publications"},{"location":"Publications/#publications","text":"","title":"Publications"},{"location":"Publications/#peer-reviewed-articles","text":"Garrett P. M. , Chiam, P-Y., & Yang, C-T. (In prep). Visual word processing efficiency for Chinese characters and English words. Sreekumar, V., Evans, N. J., Garrett, P. M. , Yim, H., Sederberg, P., & Dennis, S. (In prep). Using an experience sampling approach to distinguish distance versus location-based processing in memory for when. Planned submission: Psychonomic Bulletin & Review . Garrett P. M. , Bennett, M., Howard, Z., Hsieh, Y-.T., Yang, C-.T., Little, D., & Eidels, A. (In prep) Cross cultural symbolic wheel of fortune: an investigation into the mental representation of digits for Chinese and English speakers. Planned submission: Frontiers . Howard, Z., Garrett, P. M. , Little, D., Townsend, J., & Eidels, A. (In review) Nice guys check twice: exhaustive processes in systems factorial technology. Psych Review . Yim, H., Garrett, P. M. , Baker, M., Sreekumar, V., & Dennis, S. (In Review). Examining dependencies among different time scales in episodic memory - an experience sampling study. Journal of memory and language . Garrett P. M. , Bennett, M., Howard, Z., Yang, C-.T., Little, D., & Eidels, A. (In review) The cost of errors: confusion analysis and the mental representation of familiar and unfamiliar digits. Acta Psychologica . Garrett, P. M. , Howard, Z., Landy, D., Houpt, J. W., & Eidels, A. Comparative estimation systems perform under severely limited workload capacity. Journal of mathematical psychology . Dennis, S., Yim, H., Garrett, P. M. , Sreekumar, V., & Stone, B. (2019). A system for collecting and analyzing experience sampling data. Behavioural research methods . Dennis, S., Yim, H., Garrett, P. M. , Sreekumar, V., & Stone, B. (2018). Privacy vs open science. Behavioural research methods . Little, D. R., Eidels, A., Houpt, J. W., Garrett P. M. , & Griffiths, D. W. (2018). Systems Factorial Technology analysis of mixture models. Journal of mathematical psychology (in submission) . Cooper, P.S., Garrett P.M. , Rennie J.L. & Karayanidis F. (2015) Task Uncertainty Can Account for Mixing and Switch Costs in Task-Switching . PLoS ONE . 10(6).","title":" Peer Reviewed Articles "},{"location":"Publications/#peer-reviewed-conference-proceedings","text":"Dennis, S., Garrett, P.M. , Yim, H., Hamm, J., Osth, A., Sreekumar, V., & Stone, B. (2019). Privacy versus Open Science. In Goel, A., Seifert, C., & Freksa, C. (Eds.) Proceedings of the 41st Annual Conference of the Cognitive Science Society . Austin, TX: Cognitive Science Society. Yim, H., Garrett, P. M. , Baker, M., & Dennis, S. (2018). Examining the independence of scales in episodic memory using experience sampling data. Proceedings of the 40th Annual Conference of the Cognitive Science Society (in submission) . Dennis, S., Yim, H., Sreekumar, V., Evans. N., Garrett, P.M. , & Sederberg, P. (2017). A hierarchical Bayesian model of \"memory for when\" based on experience sampling data . Proceedings of the 39th Annual Conference of the Cognitive Science Society . Tillman, G., Howard, Z., Garrett, P.M. , & Eidels, A. (2017). The Stroop Effect From a Mixture of Reading Processes: A Fixed-Point Analysis . Proceedings of the 39th Annual Conference of the Cognitive Science Society .","title":" Peer Reviewed Conference Proceedings "},{"location":"Publications/#presentations","text":"","title":" Presentations "},{"location":"Publications/#conference-presentations","text":"Garrett, P. M. , Howard, Z., Houpt, J. W., Landy, D., & Eidles, A. (2019). The comparison of large and small item-sets. The annual meeting of the Australian mathematical society, Melbourne. Garrett, P. M.* , Howard, Z., Houpt, J. W., Landy, D., & Eidles, A. (2018). Estimating multiple item-sets: Harder than you think! Sydney Postgraduate Psychology Conference, University of New South Wales. Winning presentation.*** Garrett, P. M. , Yim, H., Sreekumar, V., Evans, N., & Dennis, S. (2018). A model for `When' in memory. Hunter medical research institute post-graduate conference, Newcastle. Garrett, P. M. , Yim, H., Sreekumar, V., Evans, N., & Dennis, S. (2018). A predictive model for `When' in human memory. Experimental Psychology Conference, Hobart. Garrett, P. M. , Houpt, J., Landy, D., & Eidels, A. (2017). Curve your enthusiasm: advancements in systems of estimation. School of Psychology, The University of Melbourne, Australia. Garrett, P. M. , Waters, L.A., & Eidels, A. (2017). How many jelly beans in two jars? Hunter medical research institute post-graduate conference, Newcastle. Garrett, P. M. , & Eidels, A. (2017). Assessing systems of estimation. The annual meeting of the Australian mathematical society, Brisbane. Garrett, P.M. , Houpt, J., Landy, D., & Eidels, A. (2016). Parallel vs Serial systems of subitizing. Meeting of the 56th Psychonomic Society, Boston. Garrett, P. M. , Houpt, J., Landy, D., & Eidels, A. (2016). Assessing systems of enumeration with SFT (Seminar). School of Psychology, Indiana University, Bloomington. Garrett, P. M. , Thorpe, A., & Eidels, A. (2016). How do you count? Hunter medical research institute post-graduate conference, Newcastle. Garrett, P. M. , & Eidels, A. (2016). A new method for assessing systems of enumeration. The annual meeting of the Australian mathematical society, Hobart.","title":"Conference Presentations"},{"location":"Publications/#workshops","text":"Garrett, P.M. (2019). The PhD and beyond! Pathways into and out of academia. Graduate researchers in psychological sciences workshop. University of Melbourne. Garrett, P.M. (2018). Claims of causality in health news. Decision making laboratory, University of Melbourne. Garrett, P.M. (2018). Mathematics in Cognitive Psychology. Computer-Assisted Research Mathematics and its Applications (CARMA): mathematical thinking workshop, Newcastle.","title":"Workshops"},{"location":"Publications/#conference-posters","text":"Garrett, P. M. , Waters, L., Landy, D., Houpt, J., & Eidels, A. (2018) How many jelly beans in two jars? Evidence for the parallel estimation of two quantities. Hunter medical research institute conference for graduate researchers. Garrett, P. M. , Thorpe, A., Landy, D., Houpt, J., & Eidels, A. (2017) A new approach to understanding systems of subitizing - the comparison of small quantities. Experimental psychology conference, Newcastle. Garrett, P. M. , Thorpe, A., Landy, D., Houpt, J., & Eidels, A. (2016). How do you count? Cognitive processing systems of enumeration. 57th annual meeting of the Psychonomics Society, Boston. Garrett, P. M. , Cooper, P., & Karayanidis, F. (2014). Using cognitive control to resolve information context predicts both global and local switch costs. Hunter medical research institute graduate researchers in Psychology conference. Garrett, P. M. , Cooper, P., & Karayanidis, F. (2014). Mid-frontal theta: a measure of attentional processes indexed during cognitive control. Hunter medical research institute satellite conference for the Australian Cognitive Neuroscience Society.","title":"Conference Posters"},{"location":"SFT/","text":"Understanding Systems Factorial Technology (SFT) Introduction On the corner of a busy road, a man waits for the lights to change. Impatient to be home, he looks down at his phone and replies to a message. Glimpsing a green signal and hearing the crossing buzz, he steps out onto the road. Our experience of the world relies upon the independent processing and seamless integration of sensory modalities. Tactile, auditory, olfactory and visual information all combine to provide a rich description of the world around us. In the above example, visual cues --- the flash of green --- and auditory cues --- the buzzing signal --- are integrated simultaneously to reach a decisions. This everyday example frames an important question for cognitive scientists: How do we unify two independent sensory streams into a single conscious experience? The following provides a brief history of response-time models in cognitive psychology and how response-time can be used to answer this complex question. Cognitive Architecture Since Donders' (1868) subtraction method, cognitive scientists have been using response times (RT) to measure cognitive processes. Donder's assumed that RT could act to measure the additive cognitive processes that underlie behaviour. For example, the cognitive processes of seeing a green light and deciding to walk. By measuring each component in isolation, and subtracting their response-times, Donder's created a measure of cognitive processing. A key assumption of Donder's work was that cognitive processes were additive . A process is additive when the total decision time is equal to the sum of each component processes. Today, additive cognitive processes go by another name, a serial processing system . Under a serial processing system, each component process must terminate before the next may begin. For example, a green walk signal must finish being processed before an auditory buzz may be evaluated. Although serial systems are necessary for many tasks, for example, one must see an object before picking it up; many cognitive process occur simultaneously in a parallel processing system . Egeth (1966) was the first to use RTs to differentiate parallel from serial processing systems. Egeth assumed that if all items within an array were processes simultaneously, (i.e., in parallel), the addition of further items would have no impact on decision-time. As such, a parallel system would predict a flat mean RT slope over increasing array sizes. By contrast, a serial system would predict a steep mean RT slope, increasing with arrays size. As an early adopter of this method, Sternberg (1966) applied these principles to examine whether short-term memory was accessed in serial or in parallel. In his task, participants were presented a list of digits followed by a probe, and were asked to report whether the probed digit was in the memory list. Sternberg found that response-times increased linearly with the length of the memory list. This additivity was used as evidence of a serial processing system, a claim propagated by many influential studies (e.g., Sternberg, 1969; Cohen, 1973; Shiffrin, 1977; Navon, 1977}, and none more influential than Treisman & Gelade (1980). In their 1980 Feature Integration Theory of Attention, Treisman and Gelade used mean RT to differentiate parallel and serial visual processing systems. They found RT was invariant across set size when participants searched a display for a singular feature (a dimension of a stimulus, such as a colour or shape), however, increased monotonicity when searching for a conjunction of features (i.e., a coloured triangle; see Figure 1). These findings suggest that a singular feature could be processed irrespective of set size, through a parallel (or over-additive) cognitive processing system. Although compelling, the findings of Treisman and Gelade ignored a key component of a system processing: efficiency. Figure 1. Illustrative example of Mean RT across set size for Feature (parallel) and Conjunction (serial) searchers, as per Treismand and Gelade's (1980) Feature Integration Theory of Attention. Workload capacity Processing efficiency or workload capacity describes the rate at which information may pass through a processing system. Workload capacity is intimately tied to the processing architecture (parallel vs serial) of a system at the mean response-time level, often resulting in the phenomenon of model mimicry. Model mimicry describes how a slow, inefficient parallel system may produce identical mean response-times as a faster serial system. The efficiency of a processing system may be described as either limited, unlimited or super in workload capacity. A limited workload capacity system slows with additional sources of information, for example, processing a green walk sign in the presence of an additional buzzing signal. An unlimited workload capacity system is unaffected by additional sources of information, and a super-capacity system speeds up with additional sources of information. The mean RT predictions of Treisman and Gelade (1980) and colleagues were formed under the assumption of unlimited workload capacity. More recent work has since thrown these assumptions into question (Townsend, 1989; 1071; 1977; 1995; 2004; Eidels et al., 2010; Houpt, 2012; Houpt et al., 2014; Garrett et al., 2018). To address the problem of model mimicry, James Townsend and colleagues (Townsend et al., 1995; 2004; 2011) have developed a theoretically driven framework and suit of mathematical tools, known as Systems Factorial Technology (SFT). Systems Factorial Technology SFT is a theoretical framework, augmented by experimental methodology, that uses response-time distributions to generate unique system-models. By comparing these theoretical models to experimental data, SFT is able to identify system properties without the confound of model mimicry. Specifically, SFT was designed to identify and assess the system properties of architecture, workload capacity, stopping-rule and channel (in)dependence. Architecture describes the time-course at which information channels are combined (parallel vs serial) and workload capacity describes the system efficiency (limited, unlimited and super). Stopping-rule describes how and when processing may terminate. A self-terminating or minimum-time stopping-rule may terminate before all sources of information are fully processed (e.g., you may terminate your search for a cookie at the moment of its detection). An exhaustive or maximum-time stopping-rule must process all sources of information before a decision can be reached (e.g., you must check all the cookie's ingredients for your friend's nut allergy; see Figure 2). Finally, channel (in)dependence describes whether information channels are stochastically separate. Independent channels provide no information to one-another. Dependent channels share information and may act to facilitate or inhibit the decision process (see Eidels et al., 2011). A coactive model describes a special case of parallel processing, where information channels sum together to reach a decision. Notably, a coactive model is unaffected by stopping-rule and predicts super workload capacity. Figure 2. Illustration of Serial, Parallel and Coactive Models. Information feeds forward through each model from input, to information or evidence accumulation, before resulting in a response generation. In the case of the serial model the dotted line indicates a minimum time or 'Self Terminating' response process strategy, where a decision is made after processing completes on the first channel accumulator. SFT is both an analysis tool set and methodological framework. SFT uses distributional analysis tools to directly assess the properties of system architecture, stopping-rule and workload capacity, under the assumption of channel independence. These analysis tools require a specific methodological design, termed the double-factorial redundant-target paradigm (DFP). The Double Factorial Paradigm Figure 3 illustrates a prototypical DFP using a dot-detection task. Here, a target is defined by any source of light, and may appear in the left (channel A) or right (channel B) location. Load, (i.e., the number of information channels), is manipulated by the presence or absence of a target. Within the target conditions exists a second manipulation of target salience, (i.e., target discriminability). A high salience (H) target is easier and faster to respond to than a low salience (L) target. Double-target cells are redundant, as either target would constitute a correct `target present' response. Together, these redundant-cells host four combinations of double-target salience: high-high (HH), high-low (HL), low-high (LH) and low-low (LL). The combined manipulation of load (target presence vs absence) and salience (discriminability high vs low) allows the specially designed analysis tools of SFT to perform independent assessments of system workload and processing architecture. Figure 3. Illustration of a double factorial design necessary for Systems Factorial Technology. The task is a redundant target paradigm. Each sector represents a unique stimulus display. For example, the top left cell represents the Double (redundant) Target, with a High-High target salience manipulation. Figure adapted from Townsend & Nozawa (1995). The Capacity Coefficient The first factorial manipulation of load allows for calculation of the capacity coefficient C( t ). The capacity coefficient measures the change in efficiency of the processing system as workload, (i.e., the number of information channels), increases. The capacity coefficient is calculated by comparing the response-time distribution for double-target trials, to the response-time distribution for trials when a target is present in each channel alone --- single-target trials. Formally, the capacity coefficient is expressed as: C( t ) = log(sAB( t )) / [log(sA( t )) + log(sB( t ))] where letters A and B refer to the processing channels when each channel operates alone, A or B, or together AB. S( t ) refers to the survivor function of the channel response-times, t is time, and log is the natural logarithm. The measured change in efficiency between the channel conditions is evaluated against predictions derived from an unlimited capacity independent parallel (UCIP) model. Under the UCIP benchmark model, an unlimited capacity system predicts C( t ) = 1. A super capacity system, one that speeds with additional workload, predicts C( t ) > 1. Finally, a limited capacity system predicts C( t ) < 1. The Mean Interaction Contrast The second factorial manipulation, that of target salience, allows for diagnosis of the system processing architecture through two measures: the mean interaction contrast and the survivor interaction contrast. The mean interaction contrast or MIC, is calculated as a double-difference of mean RT between the four factorial combinations of salience. Formally, the MIC may be written as: MIC = mHH - mHL - mLH + mLL where m denotes a mean response-time, and the letters H and L denote the display salience as combinations of high (H; i.e., bright dot) and low (L; i.e., dull dot) double-target salience-conditions. Thus, HH indicates a trial with two salient targets (two bright dots), HL and LH indicate a trial with one salient and one dull target item, and LL indicates a trial with two dull target items. As high salience targets should be responded to faster than low-salience targets, correct MIC interpretation requires the following ordering: mHH < mHL, mLH < mLL. Under the assumption of a UCIP model, and correct ordering of the mean RTs, a parallel minimum-time model predicts an over-additive MIC > 0, a parallel maximum-time model predicts an under-additive MIC < 0 and all serial models predict an additive MIC = 0. These three predictions allow the MIC to easily differentiate between parallel and serial models. To further diagnose stopping-rule, we must turn to the survivor interaction contrast (SIC). The Survivor Interaction Contrasts The SIC is a contrast measure, similar to the MIC, but calculated from the survivor functions of each double-target salience combination. It is defined as: SIC( t ) = sHH( t ) - sHL( t ) - sLH( t ) + sLL( t ) Different models predict unique SIC( t ) functions, as illustrated in Figure 4. A necessary assumption for valid interpretation of the SICSIC( t ) is the assumption of selective influence and ordering of the composite salience conditions such that sHH( t ) < sHL( t ), sLH( t ) < sLL( t ). Fortunately, these survivor functions are easily subjected to non-parametric tests. Appropriate application of the SIC( t ) and MIC allows for comprehensive diagnosis of system architecture and stopping rule within the double-target condition. Figure 4. Illustration of the five SIC models predicted by the unique combinations of processing architecture and stopping rule. Note, coactive models are identical under either stopping rule and can be identified by the combination of SIC( t ) and MIC. By combining the distributional system measures of SFT, specifically the SIC( t ) and C( t ), Systems Factorial Technology is able to independently diagnose processing architecture, stopping-rule and workload capacity without the confound of model mimicry. Understanding SFT and the system tools therein is difficult for new scientists. To help with this process, an introductory paper titled SFT explained to humans have been written, and an R package has been created by Joe Houpt. To further assist in this process, I have also created a small SFT in Matlab package that provides functions to generate simulations, analyse and plot SFT data (e.g., Figure 4). The section 'SFT in Matlab' will detail these efforts (tba).","title":"Systems Factorial Technology"},{"location":"SFT/#understanding-systems-factorial-technology-sft","text":"","title":"Understanding Systems Factorial Technology (SFT)"},{"location":"SFT/#introduction","text":"On the corner of a busy road, a man waits for the lights to change. Impatient to be home, he looks down at his phone and replies to a message. Glimpsing a green signal and hearing the crossing buzz, he steps out onto the road. Our experience of the world relies upon the independent processing and seamless integration of sensory modalities. Tactile, auditory, olfactory and visual information all combine to provide a rich description of the world around us. In the above example, visual cues --- the flash of green --- and auditory cues --- the buzzing signal --- are integrated simultaneously to reach a decisions. This everyday example frames an important question for cognitive scientists: How do we unify two independent sensory streams into a single conscious experience? The following provides a brief history of response-time models in cognitive psychology and how response-time can be used to answer this complex question.","title":"Introduction"},{"location":"SFT/#cognitive-architecture","text":"Since Donders' (1868) subtraction method, cognitive scientists have been using response times (RT) to measure cognitive processes. Donder's assumed that RT could act to measure the additive cognitive processes that underlie behaviour. For example, the cognitive processes of seeing a green light and deciding to walk. By measuring each component in isolation, and subtracting their response-times, Donder's created a measure of cognitive processing. A key assumption of Donder's work was that cognitive processes were additive . A process is additive when the total decision time is equal to the sum of each component processes. Today, additive cognitive processes go by another name, a serial processing system . Under a serial processing system, each component process must terminate before the next may begin. For example, a green walk signal must finish being processed before an auditory buzz may be evaluated. Although serial systems are necessary for many tasks, for example, one must see an object before picking it up; many cognitive process occur simultaneously in a parallel processing system . Egeth (1966) was the first to use RTs to differentiate parallel from serial processing systems. Egeth assumed that if all items within an array were processes simultaneously, (i.e., in parallel), the addition of further items would have no impact on decision-time. As such, a parallel system would predict a flat mean RT slope over increasing array sizes. By contrast, a serial system would predict a steep mean RT slope, increasing with arrays size. As an early adopter of this method, Sternberg (1966) applied these principles to examine whether short-term memory was accessed in serial or in parallel. In his task, participants were presented a list of digits followed by a probe, and were asked to report whether the probed digit was in the memory list. Sternberg found that response-times increased linearly with the length of the memory list. This additivity was used as evidence of a serial processing system, a claim propagated by many influential studies (e.g., Sternberg, 1969; Cohen, 1973; Shiffrin, 1977; Navon, 1977}, and none more influential than Treisman & Gelade (1980). In their 1980 Feature Integration Theory of Attention, Treisman and Gelade used mean RT to differentiate parallel and serial visual processing systems. They found RT was invariant across set size when participants searched a display for a singular feature (a dimension of a stimulus, such as a colour or shape), however, increased monotonicity when searching for a conjunction of features (i.e., a coloured triangle; see Figure 1). These findings suggest that a singular feature could be processed irrespective of set size, through a parallel (or over-additive) cognitive processing system. Although compelling, the findings of Treisman and Gelade ignored a key component of a system processing: efficiency. Figure 1. Illustrative example of Mean RT across set size for Feature (parallel) and Conjunction (serial) searchers, as per Treismand and Gelade's (1980) Feature Integration Theory of Attention.","title":"Cognitive Architecture"},{"location":"SFT/#workload-capacity","text":"Processing efficiency or workload capacity describes the rate at which information may pass through a processing system. Workload capacity is intimately tied to the processing architecture (parallel vs serial) of a system at the mean response-time level, often resulting in the phenomenon of model mimicry. Model mimicry describes how a slow, inefficient parallel system may produce identical mean response-times as a faster serial system. The efficiency of a processing system may be described as either limited, unlimited or super in workload capacity. A limited workload capacity system slows with additional sources of information, for example, processing a green walk sign in the presence of an additional buzzing signal. An unlimited workload capacity system is unaffected by additional sources of information, and a super-capacity system speeds up with additional sources of information. The mean RT predictions of Treisman and Gelade (1980) and colleagues were formed under the assumption of unlimited workload capacity. More recent work has since thrown these assumptions into question (Townsend, 1989; 1071; 1977; 1995; 2004; Eidels et al., 2010; Houpt, 2012; Houpt et al., 2014; Garrett et al., 2018). To address the problem of model mimicry, James Townsend and colleagues (Townsend et al., 1995; 2004; 2011) have developed a theoretically driven framework and suit of mathematical tools, known as Systems Factorial Technology (SFT).","title":"Workload capacity"},{"location":"SFT/#systems-factorial-technology","text":"SFT is a theoretical framework, augmented by experimental methodology, that uses response-time distributions to generate unique system-models. By comparing these theoretical models to experimental data, SFT is able to identify system properties without the confound of model mimicry. Specifically, SFT was designed to identify and assess the system properties of architecture, workload capacity, stopping-rule and channel (in)dependence. Architecture describes the time-course at which information channels are combined (parallel vs serial) and workload capacity describes the system efficiency (limited, unlimited and super). Stopping-rule describes how and when processing may terminate. A self-terminating or minimum-time stopping-rule may terminate before all sources of information are fully processed (e.g., you may terminate your search for a cookie at the moment of its detection). An exhaustive or maximum-time stopping-rule must process all sources of information before a decision can be reached (e.g., you must check all the cookie's ingredients for your friend's nut allergy; see Figure 2). Finally, channel (in)dependence describes whether information channels are stochastically separate. Independent channels provide no information to one-another. Dependent channels share information and may act to facilitate or inhibit the decision process (see Eidels et al., 2011). A coactive model describes a special case of parallel processing, where information channels sum together to reach a decision. Notably, a coactive model is unaffected by stopping-rule and predicts super workload capacity. Figure 2. Illustration of Serial, Parallel and Coactive Models. Information feeds forward through each model from input, to information or evidence accumulation, before resulting in a response generation. In the case of the serial model the dotted line indicates a minimum time or 'Self Terminating' response process strategy, where a decision is made after processing completes on the first channel accumulator. SFT is both an analysis tool set and methodological framework. SFT uses distributional analysis tools to directly assess the properties of system architecture, stopping-rule and workload capacity, under the assumption of channel independence. These analysis tools require a specific methodological design, termed the double-factorial redundant-target paradigm (DFP).","title":"Systems Factorial Technology"},{"location":"SFT/#the-double-factorial-paradigm","text":"Figure 3 illustrates a prototypical DFP using a dot-detection task. Here, a target is defined by any source of light, and may appear in the left (channel A) or right (channel B) location. Load, (i.e., the number of information channels), is manipulated by the presence or absence of a target. Within the target conditions exists a second manipulation of target salience, (i.e., target discriminability). A high salience (H) target is easier and faster to respond to than a low salience (L) target. Double-target cells are redundant, as either target would constitute a correct `target present' response. Together, these redundant-cells host four combinations of double-target salience: high-high (HH), high-low (HL), low-high (LH) and low-low (LL). The combined manipulation of load (target presence vs absence) and salience (discriminability high vs low) allows the specially designed analysis tools of SFT to perform independent assessments of system workload and processing architecture. Figure 3. Illustration of a double factorial design necessary for Systems Factorial Technology. The task is a redundant target paradigm. Each sector represents a unique stimulus display. For example, the top left cell represents the Double (redundant) Target, with a High-High target salience manipulation. Figure adapted from Townsend & Nozawa (1995).","title":"The Double Factorial Paradigm"},{"location":"SFT/#the-capacity-coefficient","text":"The first factorial manipulation of load allows for calculation of the capacity coefficient C( t ). The capacity coefficient measures the change in efficiency of the processing system as workload, (i.e., the number of information channels), increases. The capacity coefficient is calculated by comparing the response-time distribution for double-target trials, to the response-time distribution for trials when a target is present in each channel alone --- single-target trials. Formally, the capacity coefficient is expressed as: C( t ) = log(sAB( t )) / [log(sA( t )) + log(sB( t ))] where letters A and B refer to the processing channels when each channel operates alone, A or B, or together AB. S( t ) refers to the survivor function of the channel response-times, t is time, and log is the natural logarithm. The measured change in efficiency between the channel conditions is evaluated against predictions derived from an unlimited capacity independent parallel (UCIP) model. Under the UCIP benchmark model, an unlimited capacity system predicts C( t ) = 1. A super capacity system, one that speeds with additional workload, predicts C( t ) > 1. Finally, a limited capacity system predicts C( t ) < 1.","title":"The Capacity Coefficient"},{"location":"SFT/#the-mean-interaction-contrast","text":"The second factorial manipulation, that of target salience, allows for diagnosis of the system processing architecture through two measures: the mean interaction contrast and the survivor interaction contrast. The mean interaction contrast or MIC, is calculated as a double-difference of mean RT between the four factorial combinations of salience. Formally, the MIC may be written as: MIC = mHH - mHL - mLH + mLL where m denotes a mean response-time, and the letters H and L denote the display salience as combinations of high (H; i.e., bright dot) and low (L; i.e., dull dot) double-target salience-conditions. Thus, HH indicates a trial with two salient targets (two bright dots), HL and LH indicate a trial with one salient and one dull target item, and LL indicates a trial with two dull target items. As high salience targets should be responded to faster than low-salience targets, correct MIC interpretation requires the following ordering: mHH < mHL, mLH < mLL. Under the assumption of a UCIP model, and correct ordering of the mean RTs, a parallel minimum-time model predicts an over-additive MIC > 0, a parallel maximum-time model predicts an under-additive MIC < 0 and all serial models predict an additive MIC = 0. These three predictions allow the MIC to easily differentiate between parallel and serial models. To further diagnose stopping-rule, we must turn to the survivor interaction contrast (SIC).","title":"The Mean Interaction Contrast"},{"location":"SFT/#the-survivor-interaction-contrasts","text":"The SIC is a contrast measure, similar to the MIC, but calculated from the survivor functions of each double-target salience combination. It is defined as: SIC( t ) = sHH( t ) - sHL( t ) - sLH( t ) + sLL( t ) Different models predict unique SIC( t ) functions, as illustrated in Figure 4. A necessary assumption for valid interpretation of the SICSIC( t ) is the assumption of selective influence and ordering of the composite salience conditions such that sHH( t ) < sHL( t ), sLH( t ) < sLL( t ). Fortunately, these survivor functions are easily subjected to non-parametric tests. Appropriate application of the SIC( t ) and MIC allows for comprehensive diagnosis of system architecture and stopping rule within the double-target condition. Figure 4. Illustration of the five SIC models predicted by the unique combinations of processing architecture and stopping rule. Note, coactive models are identical under either stopping rule and can be identified by the combination of SIC( t ) and MIC. By combining the distributional system measures of SFT, specifically the SIC( t ) and C( t ), Systems Factorial Technology is able to independently diagnose processing architecture, stopping-rule and workload capacity without the confound of model mimicry. Understanding SFT and the system tools therein is difficult for new scientists. To help with this process, an introductory paper titled SFT explained to humans have been written, and an R package has been created by Joe Houpt. To further assist in this process, I have also created a small SFT in Matlab package that provides functions to generate simulations, analyse and plot SFT data (e.g., Figure 4). The section 'SFT in Matlab' will detail these efforts (tba).","title":"The Survivor Interaction Contrasts"}]}